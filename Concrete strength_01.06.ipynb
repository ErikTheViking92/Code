{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0190fded-5804-4fec-824e-664c3db81c9d",
      "metadata": {},
      "source": [
        "# **Predicting Concrete Compressive Strength**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927116cb-12cc-4c5e-96e3-05583307ab34",
      "metadata": {},
      "source": [
        "### Motivation:\n",
        "Concrete is a fundamental material in civil engineering. Its compressive strength is important for ensuring the safety and durability of structures. However, determining this strength accurately is challenging because it depends on a nonlinear function between several factors, including the age of the concrete and the proportions of its components—such as cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n",
        "This project aims to apply machine learning techniques to improve the prediction of concrete strength, in order to contribute to better design, quality control, and resource optimization in construction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2383ccde-4789-4114-99e4-58649a870fa4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Dataset Explanation:\n",
        "The dataset is sourced from the UCI Machine Learning Repository and contains 1030 observations with 8 quantitative input variables, and 1 quantitative output variable.\n",
        "The input variables are: cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate and age. Except for age, the other variables are ingredient quantities in the concrete mixture and measured in *kg in a $m^{3}$* mixture. The age is measured in *days*.\n",
        "The target, concrete compressive strengt, is measured in *MPa*.\n",
        "\n",
        "The dataset is available under: https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6142253a-0d1a-4536-9cc5-cc866ec2ac83",
      "metadata": {},
      "source": [
        "### Data Analytics: Preprocessing\n",
        "Since there is no missing attribute values according to the information provided by UCI Machine Learning Repository. Data can be directly read from Concrete_data.csv. "
      ]
    },
    {
      "cell_type": "code",
      "id": "fb071974-026d-4697-9690-3ed22b465284",
      "metadata": {},
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import sklearn\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas.plotting import scatter_matrix\n",
        "from itertools import combinations\n",
        "from IPython.display import display, Math\n",
        "\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_validate\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot') \n",
        "sns.set_style(\"darkgrid\")\n",
        "# print numpy arrays with precision 4\n",
        "np.set_printoptions(precision=4)"
      ]
    },
    {
      "cell_type": "code",
      "id": "c5280c72-0c39-4730-b6ec-60bbc2ef37c4",
      "metadata": {},
      "source": [
        "concrete_df = pd.read_csv('./Concrete_data.csv', \n",
        "                          sep = ',',\n",
        "                         decimal = ',',\n",
        "                         encoding = 'UTF-8')\n",
        "concrete_df.columns = concrete_df.columns.str.strip()\n",
        "concrete_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "id": "5e5c4fba-d88c-48f9-95e7-3564cbe0819f",
      "metadata": {},
      "source": [
        "print(\"Original data types：\")\n",
        "print(concrete_df.dtypes)\n",
        "\n",
        "# object to numeric\n",
        "for col in concrete_df.columns:\n",
        "    if concrete_df[col].dtype == 'object':\n",
        "        concrete_df[col] = pd.to_numeric(concrete_df[col], errors='coerce')\n",
        "\n",
        "\n",
        "print(\"\\nTransfromed datatypes：\")\n",
        "print(concrete_df.dtypes)\n",
        "\n",
        "# NaN \n",
        "print(\"\\nRows with NaN：\", concrete_df.isna().sum().sum())\n",
        "\n",
        "# Drop NaN\n",
        "concrete_df = concrete_df.dropna()\n",
        "concrete_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "id": "36782121-6a11-4d5c-9d2f-bad049c3e982",
      "metadata": {},
      "source": [
        "#define all the variables we need for the linear regression with concrete compressive strength as response and all other variables as possible features\n",
        "label_column = 'Concrete compressive strength(MPa, megapascals)'\n",
        "feature_columns = [c for c in concrete_df.columns if c != label_column]\n",
        "X = concrete_df[feature_columns].values\n",
        "y = concrete_df[label_column].values\n",
        "p = len(feature_columns)\n",
        "print(\"Features are:\", feature_columns,\n",
        "      \", and the response variable is:\", label_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e18e9dc-e27b-45f0-bc16-3bc41f377da1",
      "metadata": {},
      "source": [
        "#### Explanation of relevant variables:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add94d5a-0be7-4a1a-a87e-f3a29608bc48",
      "metadata": {},
      "source": [
        "The following table shows the meaning and expected effect of respective variables on the concrete compressive strength. \n",
        "| Variable             | Meaning                              | Expected Effect on Strength    |\n",
        "| -------------------- | ------------------------------------ | ------------------------------ |\n",
        "| `cement`             | Binder                               | increases strength          |\n",
        "| `blast_furnace_slag` | Supplementary binder                 | increases strength or neutral                  |\n",
        "| `fly_ash`            | Supplementary binder                 | increase strength or neutral                  |\n",
        "| `water`              | Affects workability and hydration    | decreases strength (excess) |\n",
        "| `superplasticizer`   | Increases workability, reduces water | increase or decrease depending on context  |\n",
        "| `coarse_aggregate`   | Filler                               | neutral or mild effect         |\n",
        "| `fine_aggregate`     | Filler                               | neutral or mild effect         |\n",
        "| `age`                | Curing time                          | increases strength          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac13e8f7-2bf1-4d47-b306-1c94d41c523a",
      "metadata": {},
      "source": [
        "#### Feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "id": "9234781f-dfb1-49e6-afb8-9c7251e7da2d",
      "metadata": {},
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e587337-71bf-4be0-a81b-9c12bd7abf76",
      "metadata": {},
      "source": [
        "#### Distribution of target variable"
      ]
    },
    {
      "cell_type": "code",
      "id": "e5c061e3-9565-4550-bd81-b2a2806e0662",
      "metadata": {},
      "source": [
        "sns.histplot(concrete_df[label_column], kde=True)\n",
        "plt.title(\"Distribution of Compressive Strength\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14aea602-c686-4f6a-888f-8bf9c49244bb",
      "metadata": {},
      "source": [
        "The sns.histplot clearly illustrates that the compressive strength data is not uniformly or normally distributed. Instead, it shows a sporadic and spread-out distribution with many unique or rarely occurring strength values, and potentially some concentrations at the extreme ends of the\n",
        "measured range.\n",
        "The data distribution is sparse, has a lack of strong central Tendency, so we need to avoid assumption-heavy models (Simple linear regression) and consider robust models (Decision-tree is expected to perform the best).\n",
        "That doesn’t mean that we don’t try simple linear regression, but maybe this can already explain why linear regression doesn’t perform well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6faa99d-e1e4-4c2f-b75f-40e963f83c13",
      "metadata": {},
      "source": [
        "#### Correlation between relevant variables:"
      ]
    },
    {
      "cell_type": "code",
      "id": "43ae9180-97fc-4e43-9aea-00cc84ff3126",
      "metadata": {},
      "source": [
        "corr = concrete_df.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8420141-c9ca-414a-89f8-a08550c534b7",
      "metadata": {},
      "source": [
        "#### Variables that influence concrete strength:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f3d5b1-6386-4095-9180-a8d3092ddd3d",
      "metadata": {},
      "source": [
        "According to the result of the heatmap, it can be seen that **cement** has a moderately strong positive correlation with concrete strength with a value of +0.50, which indicates more cement usually leads to stronger concrete. **Superplasticizer** also has a positive effect with a value of +0.37.\n",
        "**Age** has a positive effect too , because concrete gets stronger over time.\n",
        "**Water** has a negative correlation with strength of -0.29, which means more water tends to weaken concrete. This observation is consistent with **Abrams' water-to-cement ratio pronouncement**, which emphasizes that increased water relative to cement typically reduces concrete strength.\n",
        "\n",
        "The other components like fly Ash, Coarse Aggregate and Fine Aggregate are weakly negatively correlated with concrete strength."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5a3353-f378-47c8-a38d-ed03b34d03d8",
      "metadata": {},
      "source": [
        "#### Multicollinearity:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17706878-5c60-444b-933b-e9476b2d19cd",
      "metadata": {},
      "source": [
        "The highest multicollinearity is the correlation between **water and superplasticizer**, which has a value of -0.66. This indicates they carry overlapping and opposite information. There is also moderate correlation among **cement**, **blast furnace slag**, and **fly ash** because these may act as partial substitutes in the concrete mix, so they tend to move in opposite directions. "
      ]
    },
    {
      "cell_type": "code",
      "id": "fdcbdc69-5325-4790-879f-451bd37f29ff",
      "metadata": {},
      "source": [
        "VIFs = [(predictor, variance_inflation_factor(X,_)) \\\n",
        "        for _,predictor in enumerate(list(feature_columns))] \n",
        "print('Variance Inflation Factors')\n",
        "for tup in VIFs:\n",
        "    print('{:20}'.format(tup[0]), '{:.3f}'.format(tup[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9edd83c-d640-43fd-b67f-4e76bcb42ac4",
      "metadata": {},
      "source": [
        "The variance inflation factors are larger for cement, blast furnace slag, fly ash, water, coarse aggregate, fine aggregate. This also indicates multi-collinearity of the factors mention above.\n",
        "\n",
        "As mentioned in the lecture content of Machine Learning and Data Analytics(MLDA), VIF equal to 1 shows the absence of collinearity Usually, there is collinearity when VIF is larger than 5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9539ce69-41fa-4218-a268-242147cc9d83",
      "metadata": {},
      "source": [
        "### Machine Learning Methods:\n",
        "This project aims to predict concrete compressive strength which is a problem with a quantitative response. According to the lecture content in the fundamental part of MLDA, this task is a **regression task**. In the following part, linear regression, ridge regression, lasso regression, one decision-tree based method are applied. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "950c77ad-6c6c-4817-b9d9-5f8dfb51fc93",
      "metadata": {},
      "source": [
        "### Cross validation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a12737-156b-4734-8418-3efea661a73c",
      "metadata": {},
      "source": [
        "#### K-fold cross validation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fb05e9-9d86-49d6-9abb-a3009d0f959e",
      "metadata": {},
      "source": [
        "For supervised machine learning, the goal is to develop a model that can perform accurately not only on the data it was trained on, but also on previously unseen data. To evaluate a model's generalization ability, the **K-fold** method is utilized as the cross validation method."
      ]
    },
    {
      "cell_type": "code",
      "id": "61ec3953-992d-4cef-aeee-0249bb12fd4d",
      "metadata": {},
      "source": [
        "# define the kfold, shuffle the training and test dataset every time\n",
        "kf = KFold(n_splits=10, random_state = 0, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d984cde4-d440-47b9-a258-0ddc16ef9eb5",
      "metadata": {},
      "source": [
        "##### Scorers for the model"
      ]
    },
    {
      "cell_type": "code",
      "id": "5aa67267-45c5-4267-a03b-a14390333751",
      "metadata": {},
      "source": [
        "# Create MSE scorer \n",
        "mse_scorer = make_scorer(mean_squared_error)\n",
        "\n",
        "# Create r2 scorer \n",
        "r2 = make_scorer(r2_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a40e51-f4b2-4cf0-96a1-ac219f6e7813",
      "metadata": {},
      "source": [
        "#### Linear Regression:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d47e7a8-2319-4656-b77d-6f18cfff9fca",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "##### Statsmodel, outliers and high leverege points:"
      ]
    },
    {
      "cell_type": "code",
      "id": "319bbb53-ecfa-4d68-8ab9-26dc74ca9201",
      "metadata": {},
      "source": [
        "#Statsmodel fitting, it is used to show the p-values\n",
        "X_stat = sm.add_constant(concrete_df.iloc[:,0:-1])\n",
        "y_stat = concrete_df[label_column]\n",
        "\n",
        "statmodel = sm.OLS(y_stat,X_stat)\n",
        "estimate = statmodel.fit()\n",
        "\n",
        "print(estimate.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a58eae6-e87b-40bd-96b3-adc78c7b6d6c",
      "metadata": {},
      "source": [
        "According to the result of the fitted model, there is a relationship between the response of concrete compressive strength and **cement, blast furnace slag, fly ash, water, superplasticizer and age**, because they have low p-values, which is smaller than 0.05."
      ]
    },
    {
      "cell_type": "code",
      "id": "aa47e639-e764-4d96-848b-e6bea5d38c17",
      "metadata": {},
      "source": [
        "# Obtain the residuals, studentized residuals and the leverages\n",
        "fitted_values = estimate.fittedvalues\n",
        "residuals = estimate.resid.values\n",
        "studentized_residuals = OLSInfluence(estimate).resid_studentized_internal\n",
        "leverages = OLSInfluence(estimate).influence"
      ]
    },
    {
      "cell_type": "code",
      "id": "8ee79d69-ad90-4a6b-a2d4-74c0ec264495",
      "metadata": {},
      "source": [
        "# Calculate thresholds\n",
        "n = len(fitted_values)\n",
        "p = X.shape[1] - 1  # exclude constant\n",
        "leverage_thresh = (p + 1) / n\n",
        "\n",
        "# Convert to arrays\n",
        "studentized_residuals = np.asarray(studentized_residuals)\n",
        "leverages = np.asarray(leverages)\n",
        "\n",
        "# Identify outlier indices\n",
        "outlier_indices = np.where(np.abs(studentized_residuals) > 3)[0]\n",
        "high_leverage_indices = np.where(leverages > leverage_thresh)[0]\n",
        "outliers = []\n",
        "for idx in outlier_indices:\n",
        "    outliers.append(idx)\n",
        "print(\"Outliers are:\",outliers)\n",
        "\n",
        "# Find common indices (both outlier and high leverage)\n",
        "joint_outliers = np.intersect1d(outlier_indices, high_leverage_indices)\n",
        "high_leverage_and_outliers = [] \n",
        "\n",
        "for idx in joint_outliers:\n",
        "    high_leverage_and_outliers.append(idx)\n",
        "print(\"Comments on joint outliers (in both ax2 and ax3):\",high_leverage_and_outliers)\n",
        "\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "# 1. Residuals plot\n",
        "ax1.scatter(fitted_values, residuals, facecolors='none', edgecolors='b')\n",
        "ax1.set_xlabel('Fitted values')\n",
        "ax1.set_ylabel('Residuals')\n",
        "\n",
        "# 2. Studentized Residuals plot\n",
        "ax2.scatter(fitted_values, studentized_residuals, facecolors='none', edgecolors='b')\n",
        "ax2.axhline(y=3, color='r', linestyle='--', linewidth=1)\n",
        "ax2.axhline(y=-3, color='r', linestyle='--', linewidth=1)\n",
        "# Mark joint outliers\n",
        "for idx in joint_outliers:\n",
        "    ax2.scatter(fitted_values[idx], studentized_residuals[idx], color='red')\n",
        "    ax2.annotate(idx, (fitted_values[idx], studentized_residuals[idx]), color='red', fontsize=8)\n",
        "ax2.set_xlabel('Fitted values')\n",
        "ax2.set_ylabel('Studentized residuals')\n",
        "\n",
        "# 3. Leverage vs Studentized Residuals\n",
        "ax3.scatter(leverages, studentized_residuals, facecolors='none', edgecolors='b')\n",
        "ax3.axhline(y=3, color='r', linestyle='--', linewidth=1)\n",
        "ax3.axhline(y=-3, color='r', linestyle='--', linewidth=1)\n",
        "ax3.axvline(x=leverage_thresh, color='g', linestyle='--', linewidth=1)\n",
        "# Mark joint outliers\n",
        "for idx in joint_outliers:\n",
        "    ax3.scatter(leverages[idx], studentized_residuals[idx], color='red')\n",
        "    ax3.annotate(idx, (leverages[idx], studentized_residuals[idx]), color='red', fontsize=8)\n",
        "ax3.set_xlabel('Leverage')\n",
        "ax3.set_ylabel('Studentized residuals')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "574fadd4-fa96-4cd0-8392-df3564aceedf",
      "metadata": {},
      "source": [
        "The residuals display a clear pattern, which suggests **non-linearity** in the data. The studentized residuals indicate the presence of two outliers. The leverage plot also reveals several high-leverage points—defined as points with leverage greater than $(p+1)/n=9/1030\\approx 0.009$. Notably, both outliers are also high-leverage points. However, since the dataset does not clearly define criteria for excluding data points, all observations, including the outliers, are retained for the subsequent analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eecc8fba-5663-4950-bc04-4c59101b9cae",
      "metadata": {},
      "source": [
        "##### Scikit-Model:"
      ]
    },
    {
      "cell_type": "code",
      "id": "26c9352e-0f90-410d-968c-86ce665e8acf",
      "metadata": {},
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "mse_scores_lin = cross_val_score(model, X, y, scoring=mse_scorer, cv=kf)\n",
        "\n",
        "r2_scores = cross_val_score(model, X, y, scoring='r2', cv=kf)\n",
        "\n",
        "print(\"\\nAverage MSE across folds:\", np.mean(mse_scores_lin))    \n",
        "\n",
        "print(\"\\nAverage R2 across folds:\", np.mean(r2_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be884505-609f-4b0c-9ed7-1b9457c31a3e",
      "metadata": {},
      "source": [
        "#### Ridge regression:"
      ]
    },
    {
      "cell_type": "code",
      "id": "0a63e48f-372f-48fa-a905-48f6428ae80a",
      "metadata": {},
      "source": [
        "scoring = {'mse':'neg_mean_squared_error','R2':'r2'}\n",
        "\n",
        "# define the model\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    PolynomialFeatures(degree=2),\n",
        "    GridSearchCV(\n",
        "        estimator = Ridge(),\n",
        "        cv = kf,\n",
        "        scoring = scoring,\n",
        "        refit = 'mse',\n",
        "        \n",
        "        # param_grid determines the parameters to test (alpha is lambda in the Ridge estimator)\n",
        "        # np.logspace(-3, 2, 50): array from 10^-3 to 10^2 in 50 steps (base default is 10, can also be something else)\n",
        "        param_grid = {'alpha': np.logspace(-3, 2, 50)},\n",
        "    )\n",
        ")\n",
        "model.fit(X, y)\n",
        "#print(model[2].cv_results_) #to show the results and names\n",
        "\n",
        "# obtain the results\n",
        "lambdas = [p['alpha'] for p in model[2].cv_results_['params']]\n",
        "mses = [neg_mse * -1 for neg_mse in model[2].cv_results_['mean_test_mse']]\n",
        "r2 = [r2 for r2 in model[2].cv_results_['mean_test_R2']]\n",
        "    \n",
        "best_model = model.named_steps['gridsearchcv'].best_estimator_\n",
        "best_mse = min(mses)\n",
        "best_r2 = max(r2)\n",
        "\n",
        "best_lambda_mse = lambdas[np.argmin(mses)]\n",
        "best_lambda_r2 = lambdas[np.argmax(r2)]\n",
        "\n",
        "print(f\"Best alpha regarding mse:{best_lambda_mse:.2f}, with MSE: {best_mse:.2f}\")\n",
        "print(f\"Best alpha regarding r2:{best_lambda_r2:.2f}, with r2: {best_r2:.2f}\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "id": "8e4c67fc-2c05-467d-b652-0a0e7955a5ec",
      "metadata": {},
      "source": [
        "# plot the reults\n",
        "fig, ax = plt.subplots(figsize = (12, 8))\n",
        "ax.plot(lambdas, mses)\n",
        "ax.set_xscale('log')\n",
        "ax.set_title(r\"MSE as a function of $\\lambda$\")\n",
        "ax.set_xlabel(r\"Hyper-parameter $\\lambda$\")\n",
        "ax.set_ylabel(\"Estimated MSE\");\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c05058f1-b274-4738-b41b-341987d58041",
      "metadata": {},
      "source": [
        "The ridge regression was evaluated across a range of λ values from $10^{-3}$ to $10^2$. The validation MSE decreased with small increases in λ due to reduced overfitting, reaching a minimum at λ = 0.45. Beyond this point, further increases in λ caused the model to underfit the data, resulting in higher MSEs.\n",
        "Thus, the optimal λ value was selected as 0.45, where the model achieved the lowest MSE of 59.34. "
      ]
    },
    {
      "cell_type": "code",
      "id": "ac05e569-3cec-4480-b971-76e17b8a13ef",
      "metadata": {},
      "source": [
        "lambdas = np.logspace(-3, 6, 50)\n",
        "coefficients = list()\n",
        "\n",
        "# for each lambda define and fit the model and save the obtained parameters in the coefficients list\n",
        "for lam in lambdas:\n",
        "    model = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        PolynomialFeatures(degree = 2),\n",
        "        Ridge(alpha = lam)\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    coefficients.append(model[2].coef_)\n",
        "    \n",
        "    \n",
        "# plot the results    \n",
        "coefficients = np.array(coefficients).T\n",
        "fig, ax = plt.subplots(figsize = (12, 10))\n",
        "\n",
        "for coef_vals in coefficients:\n",
        "    ax.plot(lambdas, coef_vals)\n",
        "\n",
        "ax.set_xlabel(r\"Hyper-parameter $\\lambda$\")\n",
        "ax.set_ylabel(r\"Value of the model coefficients $\\beta_i$\")\n",
        "ax.set_xscale('log');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7471f06-b00b-4a8c-a179-5cbfab50a025",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "#### Polynomial Regression:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50af8a73-f235-4217-8827-ead5ff869205",
      "metadata": {},
      "source": [
        "Since there is non-linearity in the data, we use the polynomial regression to train the dataset."
      ]
    },
    {
      "cell_type": "code",
      "id": "a49577e8-d7ae-45f0-b7fc-df7b869e8db0",
      "metadata": {},
      "source": [
        "# Store results\n",
        "results_mse = {}\n",
        "results_r2 = {}\n",
        "\n",
        "for feature in feature_columns:\n",
        "    X_p = concrete_df[[feature]].values\n",
        "    #Store MSE,R2\n",
        "    mse_poly = []\n",
        "    r2_poly = []\n",
        "    for degree in range(1, 6):  # Degrees 1 to 5\n",
        "                # Create polynomial features\n",
        "                poly = PolynomialFeatures(degree=degree)\n",
        "                X_poly = poly.fit_transform(X_p)\n",
        "        \n",
        "                # Create linear regression model\n",
        "                model = LinearRegression()\n",
        "        \n",
        "                # Perform shuffled 10-fold CV and get negative MSE (sklearn returns negative MSE by default)\n",
        "                mse_scores = cross_val_score(model, X_poly, y, scoring=mse_scorer, cv=kf)\n",
        "        \n",
        "                # Store average MSE\n",
        "                mse_poly.append(np.mean(mse_scores))\n",
        "\n",
        "                # Store average R2\n",
        "                r2_poly.append(np.mean(r2_scores))    \n",
        "        # Store MSEs for this feature\n",
        "    results_mse[feature] = mse_poly\n",
        "    results_r2[feature] = r2_poly\n",
        "    # Plot results\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "best_comb = {}\n",
        "best_comb = {}\n",
        "\n",
        "# Plot MSE\n",
        "for feature, mse_list in results_mse.items():\n",
        "    plt.plot(range(1, 6), mse_list, marker='o', label=feature)\n",
        "    \n",
        "    # Sort MSEs with their degree indices\n",
        "    sorted_mse = sorted((mse, i+1) for i, mse in enumerate(mse_list))  # (MSE, degree)\n",
        "    \n",
        "    min_mse, best_degree = sorted_mse[0]\n",
        "    best_comb[feature] = best_degree\n",
        "    \n",
        "    print(f\"{feature}:\")\n",
        "    print(f\"  Lowest MSE = {min_mse:.2f} at degree {best_degree}\")\n",
        "print(best_comb)\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Polynomial Regression MSE (Each Feature Individually)')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "a341a859-4822-4819-ab6b-ba9d9fb21a1b",
      "metadata": {},
      "source": [
        "# not finished yet, try to combine the polynomial function\n",
        "mse_scores = []\n",
        "r2_scores = []\n",
        "\n",
        "# For plotting\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# define feature and degree\n",
        "poly_parts_list = []\n",
        "feature_names = []\n",
        "\n",
        "# Threshold, near to 0\n",
        "threshold = 1e-4\n",
        "equation = f\"y = {intercept:.4f}\"\n",
        "current_idx = 0\n",
        "\n",
        "# Begin K-Fold loop\n",
        "for feature, degree in best_comb.items():\n",
        "    \n",
        "    X_p = concrete_df[[feature]].values\n",
        "    \n",
        "    X_poly_parts = X_p ** degree\n",
        "    \n",
        "    poly_parts_list.append(X_poly_parts.reshape(-1, 1))\n",
        "    feature_names.append(f\"{feature}^{degree}\")\n",
        "    \n",
        "#combine all the feature^degree    \n",
        "X_poly_comb = np.hstack(poly_parts_list)\n",
        "print(X_poly_comb.shape)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly_comb, y)\n",
        "\n",
        "#intercept results\n",
        "intercept = model.intercept_\n",
        "coefs = model.coef_\n",
        "\n",
        "#polynomial equation\n",
        "equation = f\"y = {intercept:.4f}\"\n",
        "for name, coef in zip(feature_names, coefs):\n",
        "    equation += f\" + {coef:.4f}*{name}\"\n",
        "print('Polynomial equation is:')\n",
        "print(equation)\n",
        "\n",
        "# Perform shuffled 10-fold CV and get negative MSE (sklearn returns negative MSE by default)\n",
        "mse_scores = cross_val_score(model, X_poly, y, scoring=mse_scorer, cv=kf)\n",
        "r2_scores = cross_val_score(model, X_poly, y, scoring=mse_scorer, cv=kf)   \n",
        "\n",
        "# Store average MSE\n",
        "mse_poly.append(np.mean(mse_scores))\n",
        "\n",
        "# Store average R2\n",
        "r2_poly.append(np.mean(r2_scores))    \n",
        "\n",
        "# Summary\n",
        "print(\"\\nK-Fold Cross-Validation Results:\")\n",
        "print(f\"  Avg MSE: {np.mean(mse_scores):.2f} ± {np.std(mse_scores):.2f}\")\n",
        "print(f\"  Avg R² : {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
        "\n",
        "# Plotting: True vs Predicted\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(all_y_true, all_y_pred, alpha=0.6, color='royalblue', edgecolors='k')\n",
        "plt.plot([min(all_y_true), max(all_y_true)],\n",
        "         [min(all_y_true), max(all_y_true)],\n",
        "         'r--', label='Perfect Prediction')\n",
        "\n",
        "plt.xlabel(\"Actual Concrete Strength\")\n",
        "plt.ylabel(\"Predicted Concrete Strength\")\n",
        "plt.title(\"K-Fold Cross-Validated Predictions\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b6d1d4-c0af-4cfa-b738-25c7e8224c5b",
      "metadata": {},
      "source": [
        "##### Polynomial Degree Selection Summary and Justification:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99cb3d47-7d0c-4684-b546-c74db7a44e3a",
      "metadata": {},
      "source": [
        "In order to model the relationship between the features and the concrete compressive strength, polynomial regression up to degree 5 is applied for each feature. The mean squared error (MSE) was computed using 10-fold cross-validation to assess the predictive performance at each polynomial degree.\n",
        "\n",
        "While some features achieved their lowest MSE at higher degrees, the improvements over lower degrees were often marginal. Since high-degree polynomial models tend to overfit—capturing noise instead of the underlying patter. Both the lowest and second-lowest MSEs are considered for each feature to guide model complexity choices."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b3125a9-2293-4963-bc1b-6ed987e16fc0",
      "metadata": {},
      "source": [
        "Below is a summary of the best MSEs and their associated degrees:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f70e5d-2e16-4955-bf0d-665788fd8e59",
      "metadata": {},
      "source": [
        "| Feature            | Lowest MSE | Degree | \n",
        "| ------------------ | ---------- | ------ | \n",
        "| Cement             | 232.98     | 1      | \n",
        "| Blast Furnace Slag | 303.89     | 5      | \n",
        "| Fly Ash            | 317.01     | 4      | \n",
        "| Water              | 248.95     | 4      | \n",
        "| Superplasticizer   | 272.21     | 1      |\n",
        "| Coarse Aggregate   | 296.42     | 2      | \n",
        "| Fine Aggregate     | 293.30     | 1      | \n",
        "| Age                | 206.15     | 4      | \n"
      ]
    },
    {
      "cell_type": "code",
      "id": "7e749760-f7a2-448f-8485-9bf9052e62ed",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "845392e9",
      "metadata": {},
      "source": [
        "### Re-sampling and Cross-Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff51f7e",
      "metadata": {},
      "source": [
        "We use <span style=\"color:red\">resampling methods</span> to estimate the variability of a linear regression fit:\n",
        "1. Draw $k$ samples from our training dataset\n",
        "2. Fit a linear regression for each sample\n",
        "3. Analyze how much the fits differ\n",
        "\n",
        "Methods: \n",
        "1. Cross-validation:\n",
        "    - model assessment: validate the performance of a method\n",
        "    - model selection: validate model flexibility\n",
        "2. Bootstrap: used to measure accuracy of the parameter estimate of accuracy of a method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff1ec920",
      "metadata": {},
      "source": [
        "To obtain a good trade-off between bias and variance during our cross-validation we opt for the k-fold CV method. The leave-one-out cross-validation (LOOCV) has a higher variance than k-fold CV ($k < n$ where $n$ is the number of total observations).\n",
        "The validation set approach which is necessary for CV overestimates the test error but k-fold CV provides an intermediate level of bias. \n",
        "Training is done on:\n",
        "$(k-1)*n/k$ \n",
        "observations.\n",
        "We choose to have no overlap between the k-fold training-sets to minimize correlation in the outputs and therefore minimize the variance of the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51d14a7",
      "metadata": {},
      "source": [
        "### Cross validation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f562b6b1",
      "metadata": {},
      "source": [
        "#### K-fold cross validation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd24eef",
      "metadata": {},
      "source": [
        "For supervised machine learning, the goal is to develop a model that can perform accurately not only on the data it was trained on, but also on previously unseen data. To evaluate a model's generalization ability, the **K-fold** method is utilized as the cross validation method."
      ]
    },
    {
      "cell_type": "code",
      "id": "4bd498df",
      "metadata": {},
      "source": [
        "# define the kfold, shuffle the training and test dataset every time\n",
        "kf = KFold(n_splits=10, random_state = 0, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd016d4b",
      "metadata": {},
      "source": [
        "##### Scorers for the model"
      ]
    },
    {
      "cell_type": "code",
      "id": "9d183088",
      "metadata": {},
      "source": [
        "# Create MSE scorer \n",
        "mse_scorer = make_scorer(mean_squared_error)\n",
        "\n",
        "# Create r2 scorer \n",
        "r2 = make_scorer(r2_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb25dc4",
      "metadata": {},
      "source": [
        "#### We compute and plot the LOOCV MSEs for polynomial regression models of degree 1 to 5 using the water feature:"
      ]
    },
    {
      "cell_type": "code",
      "id": "73b0823d",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "loocv_mse = []\n",
        "\n",
        "for d in degrees:\n",
        "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression()\n",
        "    loo = LeaveOneOut()\n",
        "    # Use negative MSE, so take the negative to get positive MSE\n",
        "    scores = cross_val_score(model, X_poly, y, cv=loo, scoring='neg_mean_squared_error')\n",
        "    mse = -scores.mean()\n",
        "    loocv_mse.append(mse)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(degrees, loocv_mse, marker='o', linestyle='-.-', color='blue')\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('LOOCV Mean Squared Error')\n",
        "plt.title('LOOCV MSE for Polynomial Regression (All Features)')\n",
        "plt.xticks(degrees)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"LOOCV MSEs for degrees 1-5:\", loocv_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347322a9",
      "metadata": {},
      "source": [
        "#### We compute and plot the 10-fold CV MSEs for polynomial regression models of degree 1 to 5 using all features:"
      ]
    },
    {
      "cell_type": "code",
      "id": "133736da",
      "metadata": {},
      "source": [
        "degrees = [1, 2, 3, 4, 5]\n",
        "kfold_mse = []\n",
        "\n",
        "for d in degrees:\n",
        "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression()\n",
        "    # Use 10-fold cross-validation\n",
        "    scores = cross_val_score(model, X_poly, y, cv=10, scoring='neg_mean_squared_error')\n",
        "    mse = -scores.mean()\n",
        "    kfold_mse.append(mse)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(degrees, kfold_mse, marker='o', linestyle='-.', color='k')\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('10-Fold CV Mean Squared Error')\n",
        "plt.title('10-Fold CV MSE for Polynomial Regression (All Features)')\n",
        "plt.xticks(degrees)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"10-Fold CV MSEs for degrees 1-5:\", kfold_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfed3b91",
      "metadata": {},
      "source": [
        "#### The results show a sudden increase in the MSE for both LOOCV and 10-Fold CV at polynomial degree 5. Unfortunately there is no clear indication for a single degree of polynomial that dominates the others. The results indicate that any degree from 1 through 4 is optimal with a MSE of 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f53c0",
      "metadata": {},
      "source": [
        "### Validation set approach: "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32d395c",
      "metadata": {},
      "source": [
        "#### We compare linear, quadratic and cubic regression models and their performance. We want to predict the concrete compressive strength from the water content first."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd96fb5a",
      "metadata": {},
      "source": [
        "1. We randomly split the dataframe into a training and a test set:"
      ]
    },
    {
      "cell_type": "code",
      "id": "d2b6a3a0",
      "metadata": {},
      "source": [
        "np.random.seed(0)\n",
        "training = np.random.choice([False, True], size=len(X))\n",
        "\n",
        "y_train = concrete_df['Concrete compressive strength(MPa, megapascals)'][training]\n",
        "y_test = concrete_df['Concrete compressive strength(MPa, megapascals)'][~training]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b2cd4d",
      "metadata": {},
      "source": [
        "2. The features with the highest positive correlation with concrete compressive strength are: cement (0.5), superplasticizer (0.37) and age (0.33). We fit a 4-dimensional model on the training set and compute the MSE with the predictions on the validation set for the age feature and then fit a 1-D linear regression model and compute the MSE in the same way for the cement and superplasticizer feature:"
      ]
    },
    {
      "cell_type": "code",
      "id": "c5a4db23",
      "metadata": {},
      "source": [
        "# Split the data into training and test sets for the age feature (4-D polynomial)\n",
        "X_train = sm.add_constant(np.column_stack((concrete_df['Age (day)'][training], concrete_df['Age (day)'][training] ** 2, concrete_df['Age (day)'][training] ** 3, concrete_df['Age (day)'][training] ** 4)))\n",
        "X_test = sm.add_constant(np.column_stack((concrete_df['Age (day)'][~training], concrete_df['Age (day)'][~training] ** 2, concrete_df['Age (day)'][~training] ** 3, concrete_df['Age (day)'][~training] ** 4)))\n",
        "\n",
        "# Fit the model\n",
        "model = sm.OLS(y_train, X_train)\n",
        "poly_results = model.fit()\n",
        "\n",
        "y_predictions_poly = poly_results.predict(X_test)\n",
        "# Calculate MSE and R2      \n",
        "mse_poly = mean_squared_error(y_test, y_predictions_poly)\n",
        "r2_poly = r2_score(y_test, y_predictions_poly)\n",
        "print(f\"Polynomial Regression MSE: {mse_poly:.4f}\\n\")\n",
        "print(f\"Polynomial Regression R2: {r2_poly:.4f}\\n\")\n",
        "\n",
        "# Split the data into training and test sets for the cement feature (linear regression)\n",
        "X_train = sm.add_constant(concrete_df['Cement (component 1)(kg in a m^3 mixture)'][training])\n",
        "X_test = sm.add_constant(concrete_df['Cement (component 1)(kg in a m^3 mixture)'][~training])\n",
        "\n",
        "# Fit the model\n",
        "model = sm.OLS(y_train, X_train)\n",
        "lin_results = model.fit()\n",
        "y_predictions_lin = lin_results.predict(X_test)\n",
        "\n",
        "# Calculate MSE and R2\n",
        "mse_lin = mean_squared_error(y_test, y_predictions_lin)\n",
        "r2_lin = r2_score(y_test, y_predictions_lin)\n",
        "print(f\"Linear Regression MSE: {mse_lin:.4f}\\n\")\n",
        "print(f\"Linear Regression R2: {r2_lin:.4f}\\n\")\n",
        "\n",
        "# Split the data into training and test sets for the superplasticizer feature (linear regression)\n",
        "X_train = sm.add_constant(concrete_df['Superplasticizer (component 5)(kg in a m^3 mixture)'][training])\n",
        "X_test = sm.add_constant(concrete_df['Superplasticizer (component 5)(kg in a m^3 mixture)'][~training])\n",
        "\n",
        "# Fit the model\n",
        "model = sm.OLS(y_train, X_train)    \n",
        "lin_results = model.fit()\n",
        "y_predictions_lin = lin_results.predict(X_test)\n",
        "\n",
        "# Calculate MSE and R2\n",
        "mse_lin = mean_squared_error(y_test, y_predictions_lin)\n",
        "r2_lin = r2_score(y_test, y_predictions_lin)\n",
        "print(f\"Linear Regression MSE: {mse_lin:.4f}\\n\")\n",
        "print(f\"Linear Regression R2: {r2_lin:.4f}\\n\")\n",
        "\n",
        "# Plot the results for age, cement, and superplasticizer\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "# Plot for Age\n",
        "ax1.scatter(concrete_df['Age (day)'][~training], y_test, color='blue', label='Actual', alpha=0.5)\n",
        "ax1.scatter(concrete_df['Age (day)'][~training], y_predictions_poly, color='red', label='Predicted', alpha=0.5)\n",
        "ax1.set_xlabel('Age (day)')\n",
        "ax1.set_ylabel('Concrete Compressive Strength (MPa)')\n",
        "ax1.set_title('Polynomial Regression Results (degree 4)')\n",
        "# Plot for Cement\n",
        "ax2.scatter(concrete_df['Cement (component 1)(kg in a m^3 mixture)'][~training], y_test, color='blue', label='Actual', alpha=0.5)\n",
        "ax2.scatter(concrete_df['Cement (component 1)(kg in a m^3 mixture)'][~training], y_predictions_lin, color='red', label='Predicted', alpha=0.5)\n",
        "ax2.set_xlabel('Cement (component 1)(kg in a m^3 mixture)')\n",
        "ax2.set_ylabel('Concrete Compressive Strength (MPa)')\n",
        "ax2.set_title('Linear Regression Results')\n",
        "# Plot for Superplasticizer\n",
        "ax3.scatter(concrete_df['Superplasticizer (component 5)(kg in a m^3 mixture)'][~training], y_test, color='blue', label='Actual', alpha=0.5)\n",
        "ax3.scatter(concrete_df['Superplasticizer (component 5)(kg in a m^3 mixture)'][~training], y_predictions_lin, color='red', label='Predicted', alpha=0.5)\n",
        "ax3.set_xlabel('Superplasticizer (component 5)(kg in a m^3 mixture)')\n",
        "ax3.set_ylabel('Concrete Compressive Strength (MPa)')\n",
        "ax3.set_title('Linear Regression Results')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a DataFrame for the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Feature': ['Age (day)', 'Cement (component 1)(kg in a m^3 mixture)', 'Superplasticizer (component 5)(kg in a m^3 mixture)'],\n",
        "    'Model': ['Polynomial Regression (degree 4)', 'Linear Regression', 'Linear Regression'],\n",
        "    'MSE': [mse_poly, mse_lin, mse_lin],\n",
        "    'R2': [r2_poly, r2_lin, r2_lin]\n",
        "})\n",
        "print(\"\\nResults DataFrame:\")\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40d6774",
      "metadata": {},
      "source": [
        "3. The feature with the highest negative correlation with the concrete compressive strength is water content. We fit a 4-dimensional regression model on the training set and compute the MSE with the predictions on the validation set (as a test set):"
      ]
    },
    {
      "cell_type": "code",
      "id": "842b13f9",
      "metadata": {},
      "source": [
        "X_train = sm.add_constant(np.column_stack((concrete_df['Water  (component 4)(kg in a m^3 mixture)'][training], concrete_df['Water  (component 4)(kg in a m^3 mixture)'][training] ** 2, concrete_df['Water  (component 4)(kg in a m^3 mixture)'][training] ** 3, concrete_df['Water  (component 4)(kg in a m^3 mixture)'][training] ** 4)))\n",
        "X_test = sm.add_constant(np.column_stack((concrete_df['Water  (component 4)(kg in a m^3 mixture)'][~training], concrete_df['Water  (component 4)(kg in a m^3 mixture)'][~training] ** 2, concrete_df['Water  (component 4)(kg in a m^3 mixture)'][~training] ** 3, concrete_df['Water  (component 4)(kg in a m^3 mixture)'][~training] ** 4)))\n",
        "\n",
        "# Fit the model\n",
        "model = sm.OLS(y_train, X_train)\n",
        "poly_results = model.fit()\n",
        "\n",
        "y_predictions_poly = poly_results.predict(X_test)\n",
        "# Calculate MSE and R2      \n",
        "mse_poly = mean_squared_error(y_test, y_predictions_poly)\n",
        "r2_poly = r2_score(y_test, y_predictions_poly)\n",
        "print(f\"Polynomial Regression MSE: {mse_poly:.4f}\\n\")\n",
        "print(f\"Polynomial Regression R2: {r2_poly:.4f}\\n\")\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(concrete_df['Water  (component 4)(kg in a m^3 mixture)'][~training], y_test, facecolors='none', edgecolors='b', label='Actual', alpha=0.5)\n",
        "plt.scatter(concrete_df['Water  (component 4)(kg in a m^3 mixture)'][~training], y_predictions_poly, color='r', label='Predicted', alpha=0.5)\n",
        "plt.xlabel('Water  (component 4)(kg in a m^3 mixture)')\n",
        "plt.ylabel('Concrete Compressive Strength (MPa)')\n",
        "plt.title('Polynomial Regression Results (degree 4)')\n",
        "plt.legend()\n",
        "plt.show()  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

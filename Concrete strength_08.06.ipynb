{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709c45b8",
   "metadata": {},
   "source": [
    "# **Predicting Concrete Compressive Strength**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64197c3",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d801f",
   "metadata": {},
   "source": [
    "Concrete is a fundamental material in civil engineering. Its compressive strength is crucial for ensuring the safety and durability of structures. However, accurately determining this strength is challenging because it depends on a non-linear relationship between several factors, including the concrete's age and the proportions of its components, such as cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate and fine aggregate.\n",
    "\n",
    "The aim of this project is to apply machine learning techniques to improve the prediction of concrete strength in order to contribute to better design, quality control and optimization of resources in construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4686791",
   "metadata": {},
   "source": [
    "## 2. Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02e184",
   "metadata": {},
   "source": [
    "To enable a meaningful analysis, this chapter begins with a brief description of the dataset, including its origin, structure, and variable definitions. In preparation for modeling, several preprocessing and exploratory steps are conducted. These include importing the raw data, correcting data types, handling missing values, defining input and target variables, scaling numerical features, and interpreting the target variable’s distribution. Additionally, relevant variables are examined and their correlations are analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81193386",
   "metadata": {},
   "source": [
    "### 2.1. Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7ed51",
   "metadata": {},
   "source": [
    "The dataset originates from the UCI Machine Learning Repository and comprises 1,030 observations, 8 quantitative input variables and 1 quantitative output variable.\n",
    "\n",
    "The input variables are `cement`, `blast furnace slag`, `fly ash`, `water`, `superplasticizer`, `coarse aggregate`, `fine aggregate` and `age`. Except for `age`, the other variables represent ingredient quantities in the concrete mixture, measured in *kg in a $m^{3}$* mixture. `Age` is measured in *days*. `Concrete compressive strength`, the target variable, is measured in *MPa*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9627cd",
   "metadata": {},
   "source": [
    "### 2.2. Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92e5a7",
   "metadata": {},
   "source": [
    "The first step of the analysis involves importing the required libraries and loading the dataset.\n",
    "\n",
    "In this case, the dataset is read from `Concrete_data.csv`. This file contains the composition of different concrete mixtures along with their corresponding compressive strength values. To ensure consistent formatting, parameters such as the field separator, decimal notation, and encoding are explicitly specified during data import. Additionally, column names are stripped of leading and trailing whitespaces for cleaner access.\n",
    "\n",
    "The table below displays the first few rows of the dataset. Each row represents a unique concrete mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880071ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import sklearn\n",
    "import graphviz\n",
    "import os\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from itertools import combinations\n",
    "from IPython.display import display, Math, Image\n",
    "\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, cross_validate, RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score, mean_absolute_error,root_mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.inspection import PartialDependenceDisplay,partial_dependence\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') \n",
    "sns.set_style(\"darkgrid\")\n",
    "# print numpy arrays with precision 4\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f38ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df = pd.read_csv('./Concrete_data.csv', \n",
    "                          sep = ',',\n",
    "                         decimal = '.',\n",
    "                         encoding = 'UTF-8')\n",
    "concrete_df.columns = concrete_df.columns.str.strip()\n",
    "concrete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fd740",
   "metadata": {},
   "source": [
    "### 2.3. Data type correction and missing value handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eedbe5",
   "metadata": {},
   "source": [
    "To ensure numerical consistency across all variables, features initially read as object types — due to locale-specific formatting or encoding — are explicitly converted to numeric data types using `pd.to_numeric`. Invalid entries that cannot be parsed are coerced into NaN values. Subsequently, a missing value check is conducted, and all rows containing NaN entries are removed from the dataset to maintain data integrity and compatibility with machine learning algorithms.\n",
    "\n",
    "The table below shows the original and converted data types of all variables. Object-type features were converted to numeric formats (e.g., float64) to enable their use in regression analysis. Finally, the number of rows containing missing values (NaN) is also reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a66d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original data types：\")\n",
    "print(concrete_df.dtypes)\n",
    "\n",
    "# object to numeric\n",
    "for col in concrete_df.columns:\n",
    "    if concrete_df[col].dtype == 'object':\n",
    "        concrete_df[col] = pd.to_numeric(concrete_df[col], errors='coerce')\n",
    "\n",
    "\n",
    "print(\"\\nTransfromed datatypes：\")\n",
    "print(concrete_df.dtypes)\n",
    "\n",
    "# NaN \n",
    "print(\"\\nRows with NaN：\", concrete_df.isna().sum().sum())\n",
    "\n",
    "# Drop NaN\n",
    "concrete_df = concrete_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafee71",
   "metadata": {},
   "source": [
    "At this stage, the dataset contains no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef25fe",
   "metadata": {},
   "source": [
    "### 2.4. Definition of input and output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfa303",
   "metadata": {},
   "source": [
    "In the following, we construct the input matrix `X` and the output vector `y` for a linear regression model. This is done by selecting `concrete compressive strength` as the target variable and treating all remaining columns as input features.\n",
    "\n",
    "To improve the clarity of visualizations and tables, the feature names are shortened by omitting component numbers and units. The original component order is preserved and displayed in a separate column for reference. Finally, the selected feature names are printed to confirm the correct setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(col):\n",
    "    # Remove parentheses and their contents (component numbers and units) from column names\n",
    "    col = re.sub(r'\\(.*?\\)', '', col)\n",
    "    col = re.sub(r'component\\s*\\d*', '', col, flags=re.IGNORECASE)\n",
    "    return col.strip().replace('  ', ' ')\n",
    "\n",
    "concrete_df.columns = [clean_column_name(col) for col in concrete_df.columns]\n",
    "concrete_df.head()\n",
    "\n",
    "# Relabel the response variable\n",
    "label_column = 'Concrete compressive strength'\n",
    "# Redefine feature columns after cleaning\n",
    "feature_columns = [c for c in concrete_df.columns if c != label_column]\n",
    "#define all the variables we need for the linear regression with concrete compressive strength as response and all other variables as possible features\n",
    "X = concrete_df[feature_columns].values\n",
    "y = concrete_df[label_column].values\n",
    "p = len(feature_columns)\n",
    "print(\"Features are:\", feature_columns,\n",
    "      \"\\nThe response variable is:\", label_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38177764",
   "metadata": {},
   "source": [
    "The input features are now interpreted as `Cement`, `Blast Furnace Slag`, `Fly Ash`, `Water`, `Superplasticizer`, `Coarse Aggregate`, `Fine Aggregate` and `Age`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1decde",
   "metadata": {},
   "source": [
    "### 2.5. Explanation of relevant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64d331",
   "metadata": {},
   "source": [
    "In order to facilitate a more profound comprehension of the function of each input variable, the table below summarizes the meaning and expected influence of each feature on concrete strength."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e5eae",
   "metadata": {},
   "source": [
    "| Variable             | Meaning                              | Expected Effect on Strength    |\n",
    "| -------------------- | ------------------------------------ | ------------------------------ |\n",
    "| `Cement`             | Binder                               | increases strength          |\n",
    "| `Blast Furnace Slag` | Supplementary binder                 | increases strength or neutral                  |\n",
    "| `Fly Ash`            | Supplementary binder                 | increase strength or neutral                  |\n",
    "| `Water`              | Affects workability and hydration    | decreases strength (excess) |\n",
    "| `Superplasticizer`   | Increases workability, reduces water | increase or decrease depending on context  |\n",
    "| `Coarse Aggregate`   | Filler                               | neutral or mild effect         |\n",
    "| `Fine Aggregate`     | Filler                               | neutral or mild effect         |\n",
    "| `Age`                | Curing time                          | increases strength          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a91e9a",
   "metadata": {},
   "source": [
    "### 2.6. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d63ace",
   "metadata": {},
   "source": [
    "To ensure comparability between features and to improve the performance of algorithms sensitive to scale, the input variables are standardized using Z-score normalization, resulting in features with zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d63ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4222a",
   "metadata": {},
   "source": [
    "### 2.7. Distribution of target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ce3c9",
   "metadata": {},
   "source": [
    "To better understand the statistical properties of the response variable (`Concrete compressive strength`), we visualize its distribution using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ce3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(concrete_df[label_column], kde=True)\n",
    "plt.title(\"Distribution of Concrete compressive Strength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45cea8",
   "metadata": {},
   "source": [
    "The histogram plot clearly indicates that the compressive strength data is neither uniformly nor normally distributed. Instead, it exhibits a sporadic and spread-out pattern, with many unique or infrequent strength values and some concentration toward the distribution’s tails.\n",
    "\n",
    "This lack of strong central tendency suggests that assumption-heavy models, such as simple linear regression, may not perform optimally. Therefore, it is advisable to consider more robust models such as decision trees. While linear regression can still be applied as a baseline, the observed distribution helps explain its potential limitations in predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f350c73",
   "metadata": {},
   "source": [
    "### 2.8. Correlation between relevant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0979030",
   "metadata": {},
   "source": [
    "To explore the linear relationships among the variables, a correlation matrix is computed and visualized using a heatmap. This analysis highlights both the correlations between input features and the target variable, as well as potential multicollinearity among the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0979030",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = concrete_df.corr(numeric_only=True) # Calculate correlation matrix\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efae5f9",
   "metadata": {},
   "source": [
    "#### 2.8.1. Variables that influence concrete strength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c0c0b",
   "metadata": {},
   "source": [
    "According to the heatmap results, `cement` has a moderately strong positive correlation with concrete strength, with a value of +0.50. This indicates that more cement usually leads to stronger concrete. `Superplasticiser` also has a positive effect, with a value of +0.37.\n",
    "`Age` also has a positive effect, as concrete strengthens over time.\n",
    "`Water` has a negative correlation with strength of -0.29, meaning more water tends to weaken concrete. This observation is consistent with **Abrams' water-to-cement ratio pronouncement**, which emphasises that an increased water-to-cement ratio typically reduces concrete strength."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a432c",
   "metadata": {},
   "source": [
    "#### 2.8.2. Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1de13",
   "metadata": {},
   "source": [
    "The highest level of multicollinearity is found in the correlation between `water` and `superplasticiser`, which has a value of -0.66. This indicates that they carry overlapping yet opposing information. There is also a moderate correlation between `cement`, `blast furnace slag` and `fly ash` because these materials may act as partial substitutes in the concrete mix and therefore tend to move in opposite directions.\n",
    "\n",
    "To gain a more precise understanding of potential collinearity among the input variables, we compute the **Variance Inflation Factor (VIF)** for each predictor. **VIF** quantifies how much the variance of a regression coefficient is inflated due to the linear dependence of that variable on the others. It serves as a diagnostic tool to detect multicollinearity, which can adversely affect the interpretability and stability of regression models. As mentioned in the Machine Learning and Data Analytics (MLDA) lecture content, a **VIF** of 1 shows an absence of collinearity, whereas collinearity usually occurs when the **VIF** is greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIFs = [(predictor, variance_inflation_factor(X,_)) \\\n",
    "        for _,predictor in enumerate(list(feature_columns))] # Calculate VIFs\n",
    "print('Variance Inflation Factors')\n",
    "for tup in VIFs:\n",
    "    print('{:20}'.format(tup[0]), '{:.3f}'.format(tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [tup[0] for tup in VIFs]\n",
    "vif_values = [tup[1] for tup in VIFs]\n",
    "\n",
    "colors = ['red' if v > 10 else 'orange' if v > 5 else 'green' for v in vif_values]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "bars = plt.barh(features, vif_values, color=colors, alpha=0.85)\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.2,  \n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             f'{width:.2f}', \n",
    "             va='center', ha='left',\n",
    "             fontsize=10)\n",
    "\n",
    "plt.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='Warning Threshold (VIF=5)')\n",
    "plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Critical Threshold (VIF=10)')\n",
    "\n",
    "plt.title('Variance Inflation Factor (VIF) Analysis', fontsize=16, pad=20)\n",
    "plt.xlabel('VIF Value', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.xlim(0, max(vif_values) * 1.15)  \n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.text(0.45, 0.88,  \n",
    "         'VIF Interpretation:\\n'\n",
    "         '• VIF < 5: Low multicollinearity (green)\\n'\n",
    "         '• 5 ≤ VIF < 10: Moderate multicollinearity (orange)\\n'\n",
    "         '• VIF ≥ 10: High multicollinearity (red)',\n",
    "         transform=plt.gca().transAxes,  \n",
    "         bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'),\n",
    "         fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548c1f1",
   "metadata": {},
   "source": [
    "As shown in the table above, the variance inflation factors (VIFs) are notably high for `coarse aggregate`, `water`,`fine aggregate`,`cement`, `superplasticizer`, `fly ash` and `blast furnace slag`. This indicates a substantial degree of multicollinearity among these input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc3bb0",
   "metadata": {},
   "source": [
    "## 3. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64af85",
   "metadata": {},
   "source": [
    "This project aims to predict concrete compressive strength which is a problem with a quantitative response. According to the lecture content in the fundamental part of MLDA, this task is a **regression task**. In the following part, cross validation, stasmodels, scikit-models and tree-based methods are applied. \n",
    "\n",
    "Before applying various regression techniques, cross-validation was adopted as a fundamental evaluation strategy to ensure model generalizability across all experiments. Several regression models were in evaluated, including **statsmodel**, **scikit-model** and **tree-based methods**. **Statsmodels** were first employed to fit a linear model and analyze diagnostic plots, which revealed the presence of outliers and high leverage points. Subsequently, **scikit-models** are applied starting with a **basic linear regression**. However, the results indicated only a moderate fit. Combined with the earlier statistical findings that suggested potential non-linearity in the data, we proceeded to **polynomial regression** to better capture the underlying structure. **Polynomial regression** yielded promising results. To further enhance model performance, additional approaches were explored, including feature selection via **forward stepwise selection** and regularization techniques such as **Ridge and Lasso**. However, **feature selection** resulted in an increased MSE and a slight drop in R² on the test set. In contrast, regularization methods such as **Ridge** and **Lasso** provided comparable improvements. In particular, **Lasso** applied a thresholding mechanism that further narrowed the gap between training and test performance, thereby enhancing model robustness. In parallel, **tree-based methods**, including **Decision Tree**, **Random Forest**, and **Gradient Boosting**, were explored to further optimize prediction accuracy. Among these, the **gradient boosting** model ultimately delivered the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93fa95",
   "metadata": {},
   "source": [
    "### 3.1. Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b4e14",
   "metadata": {},
   "source": [
    "#### 3.1.1 Validation set approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2e93c",
   "metadata": {},
   "source": [
    "In supervised machine learning, the objective is to develop a model that performs well not only on the training data but also on previously unseen data. To evaluate this generalization ability, the dataset is split into a **training set** (80%) and a **test set** (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed459d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecbccf",
   "metadata": {},
   "source": [
    "#### 3.1.2. K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe6acf",
   "metadata": {},
   "source": [
    "To evaluate the model more reliably, the **K-fold method** is used. In this approach, the data is split into 10 parts; each part is used once as the validation set while the remaining parts form the training set. The results from all K iterations are then averaged to reduce the bias from a single split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f395c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the kfold, shuffle the training and test dataset every time\n",
    "kf = KFold(n_splits=10, random_state = 0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a359c5",
   "metadata": {},
   "source": [
    "#### 3.1.3. Scorers for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bad04",
   "metadata": {},
   "source": [
    "To evaluate the performance of the model during cross-validation, two standardized scoring metrics are employed: the **Mean Squared Error (MSE)** and the **coefficient of determination**, denoted as **R²**. The **MSE** quantifies the average of the squared differences between the predicted and the actual values, with lower values indicating a more accurate model fit. In contrast, the **R² score** assesses the proportion of variance in the dependent variable that is predictable from the independent variables, where values approaching 1 imply stronger explanatory and predictive capabilities. To ensure consistency in their application within the evaluation framework, both metrics are encapsulated using the `make_scorer()` function, thereby facilitating their integration into subsequent model assessment procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MSE scorer \n",
    "mse_scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "# Create r2 scorer \n",
    "r2 = make_scorer(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5f623",
   "metadata": {},
   "source": [
    "#### 3.1.4. Define a dataframe with all evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8b3f2",
   "metadata": {},
   "source": [
    "Create a DataFrame named `model_results_df` and define a function to append each model’s evaluation results for subsequent analysis.\n",
    "This provides a structured and consistent way to collect evaluation metrics, enabling efficient comparison across multiple models. The inclusion of the `R2_Gap` metric further helps identify potential overfitting or underfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Model', 'R2', 'MSE', 'Test_R2', 'Test_MSE', 'R2_Gap']\n",
    "model_results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# define a funktion to append the result into model_results_df\n",
    "def record_model_results(results_df, model_name, train_r2, train_mse, test_r2, test_mse):\n",
    "    \n",
    "    r2_gap = train_r2 - test_r2\n",
    "    \n",
    "    new_row = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'R2': train_r2,\n",
    "        'MSE': train_mse,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_MSE': test_mse,\n",
    "        'R2_Gap': r2_gap\n",
    "    }])\n",
    "    \n",
    "    if model_name in results_df['Model'].values:\n",
    "        idx = results_df.index[results_df['Model'] == model_name].tolist()[0]\n",
    "        results_df.loc[idx] = new_row.iloc[0]\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98567950",
   "metadata": {},
   "source": [
    "### 3.2. Explanation of the dataset with statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44baf2bb",
   "metadata": {},
   "source": [
    "#### 3.2.1. Ordinary Least Squares (OLS) model fitting and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a1046",
   "metadata": {},
   "source": [
    "To gain statistical insights into the impact of individual input features on concrete compressive strength, an **Ordinary Least Squares (OLS)** regression model is fitted using the statsmodels library. \n",
    "\n",
    "To prepare the data for OLS regression, the feature matrix `X_stat` is constructed by selecting all input variables from the dataset and appending a constant term to account for the intercept. The target variable `y_stat` corresponds to the concrete compressive strength. The results of the regression analysis — estimated coefficients, standard errors, and statistical significance — are summarized in the table below, based on the OLS method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsmodel fitting, it is used to show the p-values\n",
    "X_stat = sm.add_constant(concrete_df.iloc[:,0:-1])\n",
    "y_stat = concrete_df[label_column]\n",
    "\n",
    "statmodel = sm.OLS(y_stat,X_stat)\n",
    "estimate = statmodel.fit()\n",
    "\n",
    "print(estimate.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d06a91",
   "metadata": {},
   "source": [
    "Based on the result of the fitted model, a statistically significant relationship is observed between `concrete compressive strength` and several input variables, namely `cement`, `blast furnace slag`, `fly ash`, `water`, `superplasticizer`, and `age`, as indicated by their respective **p-values** being below the conventional threshold of **0.05**.\n",
    "\n",
    "The fitted OLS model yields an **R-squared value** of approximately 0.62, indicating that around 62% of the variance in concrete compressive strength can be explained by the selected predictors. This suggests a moderate level of explanatory power. The **adjusted R-squared**, which accounts for the number of predictors in the model, supports this conclusion. The **F-statistic** is relatively high, and the **corresponding p-value** is close to zero, confirming that the model as a whole is statistically significant and that the predictors are jointly informative.\n",
    "\n",
    "However, some diagnostic indicators point to potential issues. The **Omnibus test** suggests minor deviations from normality in the residuals, while the **Durbin-Watson statistic** is below 2, indicating the presence of positive autocorrelation. Although these issues are not severe, they may affect the model's reliability. \n",
    "\n",
    "Consequently, in the next step, regularized regression techniques such as Ridge or Lasso will be explored to mitigate multicollinearity and enhance predictive performance. Prior to that, all features will be standardized, and the models will be implemented using **scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0f64c",
   "metadata": {},
   "source": [
    "#### 3.2.2. Outliers and high leverage points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026d4f5",
   "metadata": {},
   "source": [
    "To assess the influence of individual data points on the fitted model, diagnostic measures are computed. In particular, the fitted values, residuals, studentized residuals, and leverage values are extracted from the OLS estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbefaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the residuals, studentized residuals and the leverages\n",
    "fitted_values = estimate.fittedvalues\n",
    "residuals = estimate.resid.values\n",
    "studentized_residuals = OLSInfluence(estimate).resid_studentized_internal\n",
    "leverages = OLSInfluence(estimate).influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da4990",
   "metadata": {},
   "source": [
    "Based on these quantities, thresholds are then defined to preliminarily identify influential observations. Data points with absolute studentized residuals greater than 3 are flagged as potential **outliers**, while those with leverage values exceeding the theoretical threshold of $(p+1)/n$ (where $p$ is the number of predictors and $n$ the sample size) are classified as **high-leverage points**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680a558",
   "metadata": {},
   "source": [
    "The three diagnostic plots respectively display:\n",
    "\n",
    "(1) **raw residuals versus fitted values**\n",
    "\n",
    "(2) **studentized residuals versus fitted values**\n",
    "\n",
    "(3) **studentized residuals versus leverage values**\n",
    "\n",
    "Each plot is used to visually identify potential outliers and high-leverage observations based on predefined statistical thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f352674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thresholds\n",
    "n = len(fitted_values)\n",
    "p = X.shape[1] - 1  # exclude constant\n",
    "leverage_thresh = (p + 1) / n\n",
    "\n",
    "# Convert to arrays\n",
    "studentized_residuals = np.asarray(studentized_residuals)\n",
    "leverages = np.asarray(leverages)\n",
    "\n",
    "# Identify outlier indices\n",
    "outlier_indices = np.where(np.abs(studentized_residuals) > 3)[0]\n",
    "high_leverage_indices = np.where(leverages > leverage_thresh)[0]\n",
    "outliers = []\n",
    "for idx in outlier_indices:\n",
    "    outliers.append(idx)\n",
    "print(\"Outliers are:\",outliers)\n",
    "\n",
    "# Find common indices (both outlier and high leverage)\n",
    "joint_outliers = np.intersect1d(outlier_indices, high_leverage_indices)\n",
    "high_leverage_and_outliers = [] \n",
    "\n",
    "for idx in joint_outliers:\n",
    "    high_leverage_and_outliers.append(idx)\n",
    "print(\"Comments on joint outliers (in both ax2 and ax3):\",high_leverage_and_outliers)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# 1. Residuals plot\n",
    "ax1.scatter(fitted_values, residuals, facecolors='none', edgecolors='b')\n",
    "ax1.set_xlabel('Fitted values')\n",
    "ax1.set_ylabel('Residuals')\n",
    "\n",
    "# 2. Studentized Residuals plot\n",
    "ax2.scatter(fitted_values, studentized_residuals, facecolors='none', edgecolors='b')\n",
    "ax2.axhline(y=3, color='r', linestyle='--', linewidth=1)\n",
    "ax2.axhline(y=-3, color='r', linestyle='--', linewidth=1)\n",
    "# Mark joint outliers\n",
    "for idx in joint_outliers:\n",
    "    ax2.scatter(fitted_values[idx], studentized_residuals[idx], color='red')\n",
    "    ax2.annotate(idx, (fitted_values[idx], studentized_residuals[idx]), color='red', fontsize=8)\n",
    "ax2.set_xlabel('Fitted values')\n",
    "ax2.set_ylabel('Studentized residuals')\n",
    "\n",
    "# 3. Leverage vs Studentized Residuals\n",
    "ax3.scatter(leverages, studentized_residuals, facecolors='none', edgecolors='b')\n",
    "ax3.axhline(y=3, color='r', linestyle='--', linewidth=1)\n",
    "ax3.axhline(y=-3, color='r', linestyle='--', linewidth=1)\n",
    "ax3.axvline(x=leverage_thresh, color='g', linestyle='--', linewidth=1)\n",
    "# Mark joint outliers\n",
    "for idx in joint_outliers:\n",
    "    ax3.scatter(leverages[idx], studentized_residuals[idx], color='red')\n",
    "    ax3.annotate(idx, (leverages[idx], studentized_residuals[idx]), color='red', fontsize=8)\n",
    "ax3.set_xlabel('Leverage')\n",
    "ax3.set_ylabel('Studentized residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fef1e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The residual plot exhibits a discernible pattern, indicating potential **non-linearity** in the data, which may violate the assumptions of the linear regression model. The studentized residuals reveal the presence of two influential observations that exceed the conventional threshold for **outlier detection**. In parallel, the leverage plot identifies several **high-leverage points** — defined as observations with leverage values exceeding the theoretical cutoff. Notably, both identified outliers also possess high leverage.\n",
    "\n",
    "However, given the absence of predefined exclusion criteria within the dataset, all observations — including these joint outliers — are retained for subsequent analysis to preserve data integrity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58257947",
   "metadata": {},
   "source": [
    "### 3.3. Prediction with scikit model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b6fd9",
   "metadata": {},
   "source": [
    "#### 3.3.1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c7db3",
   "metadata": {},
   "source": [
    "To evaluate the performance of a linear regression model, the data is first preprocessed by scaling features. A pipeline combining this preprocessing step with linear regression is then constructed. The model's predictive accuracy is assessed via K-fold cross-validation, reporting the average MSE and coefficient of determination (R²) across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf6bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data by scaling features\n",
    "scaler = StandardScaler()\n",
    "model = make_pipeline(scaler, LinearRegression())\n",
    "\n",
    "mse_scores_lin = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n",
    "r2_scores = cross_val_score(model, X_train, y_train, scoring='r2', cv=kf)\n",
    "\n",
    "print(\"\\nAverage MSE across folds:\", -np.mean(mse_scores_lin))\n",
    "print(\"\\nAverage R2 across folds:\", np.mean(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539a97b",
   "metadata": {},
   "source": [
    "Next, the **validation set approach** is employed. The model is first trained on the training set, and predictions are subsequently made on both the training and test sets. The MSE and R² are computed for each set, providing insight into the model’s fit on the training data as well as its ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb86122",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "y_pred = linear.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d9916",
   "metadata": {},
   "source": [
    "While the test MSE indicates an acceptable level of prediction error, the R² score of 0.637 reveals that the linear regression model only moderately fits the data. Although a significant portion of the variance is explained, the result also suggests that linear relationships alone may not fully capture the complexity of the target variable. To address this, polynomial features will be introduced to model potential non-linear patterns, followed by regularized regression techniques such as Ridge and Lasso to improve generalization and mitigate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d948e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the result to Dataframe\n",
    "model_name = 'Basic Linear Regression'\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    train_r2,\n",
    "    train_mse,\n",
    "    test_r2,\n",
    "    test_mse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e7ddb",
   "metadata": {},
   "source": [
    "#### 3.3.2. Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a75252",
   "metadata": {},
   "source": [
    "In this section, polynomial regression is used because there is non-linearity between the features and the target variable. To prevent overfitting due to using excessively high polynomial degrees, we evaluate models with varying degrees and select the one with the lowest MSE. This optimal degree is then used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try polynomial features with degree 2 to 4\n",
    "# Store MSE scores for each degree\n",
    "degree_mse = {}\n",
    "\n",
    "# Loop through polynomial degrees from 2 to 4\n",
    "for degree in range(2, 5):\n",
    "    model_poly = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('linreg', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    mse_scores = cross_val_score(model_poly, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n",
    "    avg_mse = -np.mean(mse_scores)\n",
    "\n",
    "    degree_mse[degree] = avg_mse\n",
    "    print(f\"Degree {degree}: Average MSE = {avg_mse:.4f}\")\n",
    "    \n",
    "\n",
    "# Find the degree with the lowest MSE\n",
    "best_degree = min(degree_mse, key=degree_mse.get)\n",
    "print(f\"\\nBest polynomial degree: {best_degree} (MSE = {degree_mse[best_degree]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b09cea",
   "metadata": {},
   "source": [
    "Among the degrees compared, the polynomial regression model with degree 3 achieved the lowest average MSE with respect to the training dataset, with a value of 50.48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = best_degree, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "# Fit the model with the best polynomial degree\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "# Prediction on the train set\n",
    "y_train_pred = model.predict(X_train_poly_scaled)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test_poly_scaled)\n",
    "\n",
    "# Calculate MSE and R2 for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "r2_gap = train_r2 - test_r2\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "print(f\"Test R2: {test_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55cdad",
   "metadata": {},
   "source": [
    "The training MSE and the test MSE are very close with training MSE equal to 50.48 and test MSE as 51.12. This indicates that the model generalizes well to unseen data and is not just memorizing the training set. Hence, there is no significant overfitting. If the model were overfitting, we would expect the training MSE to be much lower than the test MSE.\n",
    "So for follwing analyzing the degree equal to 3 will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b787573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the result to Dataframe\n",
    "model_name = f\"Polynomial Regression (degree: {best_degree})\"\n",
    "\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    train_r2,\n",
    "    train_mse,\n",
    "    test_r2,\n",
    "    test_mse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9388c3b",
   "metadata": {},
   "source": [
    "#### 3.3.3. Forward stepwise selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7100cf",
   "metadata": {},
   "source": [
    "In this step, we perform a forward stepwise selection to determine the best set of features at first. As the criterion for the comparison of the different models, we also estimate the metrics using 10-fold cross validation in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_stepwise_selection_mse_r2(X, y, max_features=None):\n",
    "    n_features = X.shape[1]\n",
    "    selected = []\n",
    "    remaining = list(range(n_features))\n",
    "    best_mse_list = []\n",
    "    best_r2_list = []\n",
    "    \n",
    "    if max_features is None:\n",
    "        max_features = n_features\n",
    "\n",
    "    while len(selected) < max_features:\n",
    "        candidates = []\n",
    "        for candidate in remaining:\n",
    "            features = selected + [candidate]\n",
    "            model = LinearRegression()\n",
    "            mse = -np.mean(cross_val_score(model, X[:, features], y, scoring='neg_mean_squared_error', cv=kf))\n",
    "            r2 = np.mean(cross_val_score(model, X[:, features], y, scoring='r2', cv=kf))\n",
    "            candidates.append((mse, r2, candidate))\n",
    "\n",
    "        # choose the minimum of mse\n",
    "        candidates.sort()\n",
    "        best_mse, best_r2, best_candidate = candidates[0]\n",
    "\n",
    "        selected.append(best_candidate)\n",
    "        remaining.remove(best_candidate)\n",
    "        best_mse_list.append(best_mse)\n",
    "        best_r2_list.append(best_r2)\n",
    "\n",
    "    return selected, best_mse_list, best_r2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial features with degree 3\n",
    "poly = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "forward_feature_names = poly.get_feature_names_out(input_features  = feature_columns)\n",
    "\n",
    "selected_features, mse_list, r2_list= forward_stepwise_selection_mse_r2(X_train_poly, y_train)\n",
    "#print(selected_features, mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_selection = []\n",
    "best_index = np.argmin(mse_list)\n",
    "for i, (mse, r2, feature_idx) in enumerate(zip(mse_list, r2_list, selected_features), 1):\n",
    "    current_selection.append(feature_idx)\n",
    "    forward_selected_names = [forward_feature_names[j] for j in current_selection]\n",
    "    if i == best_index + 1:\n",
    "        print(f\"Best MSE: {mse:.4f} | Best R²: {r2:.4f} | Features: {forward_selected_names}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train_poly[:, current_selection], y_train)\n",
    "y_pred = model.predict(X_test_poly[:, current_selection])\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" Test MSE: {test_mse:.4f} | Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208e82e",
   "metadata": {},
   "source": [
    "The model achieved its best performance with a training MSE of 33.0234 and a training R² score of 0.8812, indicating strong predictive accuracy on the training data. For this best-performing model , 66 features were used, many of which are polynomial combinations and interaction terms derived from the original variables. Notable examples include polynomial features such as Age², Age³, Superplasticizer³, and Water², as well as interaction terms like Cement × Fly Ash × Age, Water × Coarse Aggregate, and Blast Furnace Slag × Fly Ash × Fine Aggregate.\n",
    "\n",
    "With these engineered features, the model achieved a test MSE of 47.5561 and a test R² of 0.8194. While the performance on the test set remains strong, the increase in MSE and the slight drop in R² compared to the training results suggest that the model may be overfitting due to the large number of features included.\n",
    "\n",
    "To address this issue, in sections 3.3.4 and 3.3.5, we apply shrinkage methods, specifically Ridge and Lasso regression, to reduce the risk of overfitting by penalizing model complexity and encouraging simpler, more generalizable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f92b7a",
   "metadata": {},
   "source": [
    "#### 3.3.4. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'neg_mean_squared_error': 'neg_mean_squared_error', 'r2': 'r2'}\n",
    "\n",
    "# define the model\n",
    "ridgemodel = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(degree = best_degree),\n",
    "    GridSearchCV(\n",
    "        estimator = Ridge(),\n",
    "        cv = kf,\n",
    "        scoring = scoring,\n",
    "        refit = 'neg_mean_squared_error', # redit the model based on the mse\n",
    "        \n",
    "        # param_grid determines the parameters to test (alpha is lambda in the Ridge estimator)\n",
    "        # np.logspace(-3, 2, 50): array from 10^-3 to 10^2 in 50 steps (base default is 10, can also be something else)\n",
    "        param_grid = {'alpha': np.logspace(-3, 2, 50)},\n",
    "    )\n",
    ")\n",
    "ridgemodel.fit(X_train, y_train)\n",
    "#print(ridgemodel[2].cv_results_) #to show the results and names\n",
    "\n",
    "# obtain the results\n",
    "lambdas = [p['alpha'] for p in ridgemodel[2].cv_results_['params']]\n",
    "mses = [neg_mse * -1 for neg_mse in ridgemodel[2].cv_results_['mean_test_neg_mean_squared_error']]\n",
    "r2 = [r2 for r2 in ridgemodel[2].cv_results_['mean_test_r2']]\n",
    "    \n",
    "best_model = ridgemodel.named_steps['gridsearchcv'].best_estimator_\n",
    "best_mse = min(mses)\n",
    "best_r2 = max(r2)\n",
    "\n",
    "best_lambda_mse = lambdas[np.argmin(mses)]\n",
    "best_lambda_r2 = lambdas[np.argmax(r2)]\n",
    "\n",
    "print(f\"Best alpha regarding mse: {best_lambda_mse:.2f}, with MSE: {best_mse:.2f}\")\n",
    "print(f\"Best alpha regarding r2: {best_lambda_r2:.2f}, with r2: {best_r2:.2f}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612774b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reults\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "ax.plot(lambdas, mses)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(r\"MSE as a function of $\\lambda$\")\n",
    "ax.set_xlabel(r\"Hyper-parameter $\\lambda$\")\n",
    "ax.set_ylabel(\"Estimated MSE\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b423095",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = ridgemodel.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_test_pred = ridgemodel.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "r2_gap = train_r2 - test_r2\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "print(f\"Test R2: {test_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9536df",
   "metadata": {},
   "source": [
    "Ridge regression was evaluated across a range of λ values from $10^{-3}$ to $10^2$. The validation MSE decreased with small increases in λ due to reduced overfitting, reaching a minimum at λ = 0.91. Beyond this point, further increases in λ caused the model to underfit the data, resulting in higher MSEs.\n",
    "Thus, the optimal λ value was selected as 0.91, where the model achieved the lowest MSE of 36.54 and a training R² score of 0.87.\n",
    "\n",
    "On the test set, the model achieved a Test MSE of 45.19 and a Test R² score of 0.83.\n",
    "Although the evaluation scores on the training set slightly dropped, the test set showed better MSE and R² values, indicating that the model's accuracy improved with Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22197795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"Ridge (alpha={best_lambda_mse:.4f})\"\n",
    "\n",
    "# append the result to Dataframe\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    train_r2,\n",
    "    train_mse,\n",
    "    test_r2,\n",
    "    test_mse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed60a52",
   "metadata": {},
   "source": [
    "#### 3.3.5. Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline with GridSearchCV\n",
    "lassomodel = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(degree = best_degree, interaction_only=False, include_bias=False),\n",
    "    GridSearchCV(\n",
    "        estimator = Lasso(max_iter = 10000, tol = 1e-4),\n",
    "        param_grid={'alpha': np.logspace(-3, 1.5, 50)},\n",
    "        scoring=scoring,\n",
    "        refit='neg_mean_squared_error',\n",
    "        cv=kf\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "lassomodel.fit(X_train, y_train)\n",
    "#print(lassomodel[2].cv_results_)\n",
    "\n",
    "# Extract CV results\n",
    "grid = lassomodel.named_steps['gridsearchcv']\n",
    "lassolambdas = [p['alpha'] for p in grid.cv_results_['params']]\n",
    "mses_lasso = [-score for score in grid.cv_results_['mean_test_neg_mean_squared_error']]  # convert from negative MSE\n",
    "r2_lasso = grid.cv_results_['mean_test_r2']\n",
    "\n",
    "# Get best values\n",
    "best_lassomodel = grid.best_estimator_\n",
    "best_lassomse = min(mses_lasso)\n",
    "best_lassor2 = max(r2_lasso)\n",
    "\n",
    "best_lambda_mse_lasso = lassolambdas[np.argmin(mses_lasso)]\n",
    "best_lambda_r2_lasso = lassolambdas[np.argmax(r2_lasso)]\n",
    "\n",
    "print(f\"Best alpha regarding mse: {best_lambda_mse_lasso:.3f}, with MSE: {best_lassomse:.2f}\")\n",
    "print(f\"Best alpha regarding r2: {best_lambda_r2_lasso:.3f}, with R2: {best_lassor2:.3f}\")\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "ax.plot(lambdas, mses_lasso)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(r\"MSE as a function of $\\lambda$\")\n",
    "ax.set_xlabel(r\"Hyper-parameter $\\lambda$\")\n",
    "ax.set_ylabel(\"Estimated MSE\");\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a120408",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lassomodel.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "print(f\"Test R2: {test_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4130bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lassomodel.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "best_index = np.argmin(mses_lasso)\n",
    "best_alpha = lassolambdas[best_index]\n",
    "model_name = f\"Lasso (alpha={best_alpha:.4f})\"\n",
    "\n",
    "# append the result to Dataframe\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    train_r2,\n",
    "    train_mse,\n",
    "    test_r2,\n",
    "    test_mse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cd5f8",
   "metadata": {},
   "source": [
    "It can be observed that the test MSE and $R^2$ scores for the Lasso regression are similar to those obtained with Ridge regression. This is likely because the chosen regularization parameter ($\\alpha = 0.010$) is relatively small, meaning the penalty applied by Lasso is weak and does not strongly constrain the model. As a result, Lasso behaves similarly to Ridge in this case.\n",
    "\n",
    "The next step is to examine the coefficients from the Lasso model to determine which ones have been shrunk exactly to zero (indicating feature elimination) and which are merely close to zero (indicating minimal contribution). This analysis will help us understand the extent of sparsity introduced by the Lasso regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405b04e",
   "metadata": {},
   "source": [
    "##### 3.3.5.1. Non-zero coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from the fitted polynomial step\n",
    "poly = lassomodel.named_steps['polynomialfeatures']\n",
    "feature_names = poly.get_feature_names_out(input_features = feature_columns)\n",
    "\n",
    "# Get final fitted Lasso model\n",
    "lasso = lassomodel.named_steps['gridsearchcv'].best_estimator_\n",
    "\n",
    "# Get coefficients\n",
    "coefs = lasso.coef_\n",
    "\n",
    "# Combine with feature names\n",
    "lasso_selected_features = [name for name, coef in zip(feature_names, coefs) if coef != 0]\n",
    "zero_features = [(name, coef) for name, coef in zip(feature_names, coefs) if coef == 0]\n",
    "\n",
    "print(\"Non-zero coefficients:\", np.sum(lasso.coef_ != 0))\n",
    "print(\"Total features:\", len(lasso.coef_))\n",
    "print(\"Zero coefficients:\", zero_features)\n",
    "\n",
    "lasso_weights = pd.Series(coefs, index=feature_names)\n",
    "lasso_weights.sort_values(key=abs, ascending=False)  # sort by absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6be30",
   "metadata": {},
   "source": [
    "From the total of 164 features, the Lasso model identified 127 with non-zero coefficients, indicating their significance and contribution to the prediction of the target variable. The remaining 37 features had their coefficients regularized to zero, effectively excluding them from the model. \n",
    "\n",
    "To further refine the model and reduce overfitting, we subsequently explored thresholds ranging from 0 to 2.5. This post-Lasso filtering aims to remove coefficients with minimal influence on the response variable, thereby enhancing model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4337a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features transformation\n",
    "poly = PolynomialFeatures(degree = best_degree, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "feature_names = poly.get_feature_names_out(feature_columns)\n",
    "\n",
    "# Scale the features and apply polynomial transformation\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "# Define the Lasso model with GridSearchCV\n",
    "best_lasso = grid.best_estimator_\n",
    "coefs = pd.Series(best_lasso.coef_, index=feature_names)\n",
    "\n",
    "# Thresholds for feature selection\n",
    "thresholds = np.linspace(0, 2.5, 50)  # 50 thresholds from 0 to 2.5\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    selected = coefs[np.abs(coefs) >= threshold]\n",
    "    selected_indices = [i for i, name in enumerate(feature_names) if name in selected.index]\n",
    "    lasso_threshold_selected_names = list(selected.index)  # get names from coef Series\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        continue  # skip if no features are selected\n",
    "\n",
    "    X_train_selected = X_train_poly_scaled[:, selected_indices]\n",
    "    X_test_selected = X_test_poly_scaled[:, selected_indices]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    # Train and test scores\n",
    "    y_train_pred = model.predict(X_train_selected)\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    y_test_pred = model.predict(X_test_selected)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"threshold\": threshold,\n",
    "        \"n_features\": len(selected_indices),\n",
    "        \"train r2\": train_r2,\n",
    "        \"test r2\": test_r2,\n",
    "        \"train mse\": train_mse,\n",
    "        \"test mse\": test_mse,\n",
    "        \"selected_features\": lasso_threshold_selected_names\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# print(results_df)\n",
    "\n",
    "#  find the best threshold based on train mse\n",
    "best_row = results_df.loc[results_df['train mse'].idxmin()]\n",
    "print(\"\\nBest Threshold (by training MSE):\")\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de0461",
   "metadata": {},
   "source": [
    "This model, selected based on minimum training MSE, uses a threshold of 0.0, meaning no coefficients were removed, It achieved a very high training $R^2 = 0.932$ and low training MSE = 19.16*, indicating an excellent fit to the training data.\n",
    "\n",
    "However, the test $R^2 = 0.806$ and test MSE = 51.12 show that, while the model performs reasonably well on unseen data, there is a noticeable gap between training and test performance. This discrepancy suggests the model is likely **overfitting**.\n",
    "\n",
    "In summary, although this model achieves the best possible fit on the training set, no feature reduction comes at the cost of reduced generalization, making it less ideal than simpler models with comparable or better test performance. So we wanted to put more value on minimizing the gap between training and test scores.\n",
    "\n",
    "To better understand how model complexity affects generalization, the next step is to visualize the $R^2$ and MSE gap between training and testing across various coefficient thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# R²\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['threshold'], results_df['train r2'], label='Train R²', marker='o')\n",
    "plt.plot(results_df['threshold'], results_df['test r2'], label='Test R²', marker='s')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Train vs Test R² by Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# MSE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_df['threshold'], results_df['train mse'], label='Train MSE', marker='o')\n",
    "plt.plot(results_df['threshold'], results_df['test mse'], label='Test MSE', marker='s')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Train vs Test MSE by Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf52100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the R² and MSE gap\n",
    "results_df['r2 gap'] = np.abs(results_df['train r2'] - results_df['test r2'])\n",
    "results_df['mse gap'] = np.abs(results_df['train mse'] - results_df['test mse'])\n",
    "\n",
    "# find the threshold with the smallest MSE gap\n",
    "most_generalizable_mse = results_df.loc[results_df['mse gap'].idxmin()]\n",
    "\n",
    "# after finding most generalizable mse\n",
    "\n",
    "print(\"\\nThreshold with smallest Train-Test mse gap:\")\n",
    "print(most_generalizable_mse)\n",
    "\n",
    "smallest_mse_gap_features = [name for name in most_generalizable_mse['selected_features']]\n",
    "print(\"\\nSelected Features at Smallest MSE Gap:\", smallest_mse_gap_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b89b03",
   "metadata": {},
   "source": [
    "The best test R² was achieved at a threshold of ~2,04, with a test R² of 0.80, train R² of 0.82, and 24 features selected.\n",
    "The smallest R² gap (between train and test) is 0.014,and the smallest MSE gap is 0.31.\n",
    "\n",
    "Compared to the original Lasso model (without thresholding), applying a coefficient threshold improves generalization by reducing noise from low-importance features.\n",
    "As shown in the R² and MSE plots, increasing the threshold gradually reduces overfitting (smaller train-test gap), up to a point.\n",
    "However, too high a threshold (>2.45) leads to performance breakdown. It can be seen from the plot R² drops sharply and MSE spikes, due to excessive feature elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf304b0",
   "metadata": {},
   "source": [
    "##### 3.3.5.2. Features comparison from forward stepwise selection and two lasso models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b39d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to set conversion for feature comparison\n",
    "set_lasso = set(lasso_selected_features)\n",
    "set_thresh = set(smallest_mse_gap_features)\n",
    "set_forward = set(forward_selected_names)\n",
    "\n",
    "\n",
    "# common features\n",
    "common_all = set_lasso & set_thresh & set_forward\n",
    "print(f\" Features selected by ALL methods:\\n{common_all}\", f\"\\n with\" , len(common_all), \"features\")\n",
    "\n",
    "# two-way intersections\n",
    "print(f\"\\n Lasso ∩ Threshold:\\n{set_lasso & set_thresh}\", f\"\\n with\" , len(set_lasso & set_thresh), \"features\")\n",
    "print(f\"\\n Lasso ∩ Forward:\\n{set_lasso & set_forward}\", f\"\\n with\" , len(set_lasso & set_forward), \"features\")\n",
    "print(f\"\\n Threshold ∩ Forward:\\n{set_thresh & set_forward}\", f\"\\n with\" , len(set_thresh & set_forward), \"features\"   )\n",
    "\n",
    "# Specific features only in one method\n",
    "print(f\"\\n Only in Lasso:\\n{set_lasso - set_thresh - set_forward}\", f\"\\n with\" , len(set_lasso - set_thresh - set_forward), \"features\")\n",
    "print(f\"\\n Only in Thresholding:\\n{set_thresh - set_lasso - set_forward}\", f\"\\n with\" , len(set_thresh - set_lasso - set_forward), \"features\")\n",
    "print(f\"\\n Only in Forward Stepwise:\\n{set_forward - set_lasso - set_thresh}\", f\"\\n with\" , len(set_forward - set_lasso - set_thresh), \"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940de59",
   "metadata": {},
   "source": [
    "Since several methods (Lasso, a custom Thresholding approach, and Forward Stepwise selection) are used in this section , this intersection approach is to compare their outcomes. The following table is used to illustrate the results of comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071b1dc",
   "metadata": {},
   "source": [
    "| **#** | **Comparison**                   | **Feature Count**                     | **Description**                                                                                                                                                                                                  |\n",
    "| ----: | -------------------------------- | ------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "|     1 | **Selected by All Methods**      | 9                                     | Features consistently selected by **Lasso**, **Thresholding**, and **Forward Stepwise**. These are the most robust and stable predictors, showing consistent importance across all selection strategies.         |\n",
    "|     2 | **Lasso ∩ Thresholding**         | 24 (shared), 0 (only in Thresholding) | Features selected by **both Lasso and Thresholding**. Since Thresholding filters the Lasso-selected features based on coefficient magnitude, it cannot contribute additional features beyond Lasso.              |\n",
    "|     3 | **Lasso ∩ Forward Stepwise**     | 52                                    | Features identified by **both Lasso and Forward Stepwise**. The agreement indicates a strong predictive signal, but not all were retained by Thresholding, suggesting some may contribute to overfitting.        |\n",
    "|     4 | **Threshold ∩ Forward Stepwise** | 9                                     | Same as the features selected by **all methods**, reinforcing their reliability. Their selection by both Thresholding and Forward Stepwise highlights their stability across both filtering and greedy methods.  |\n",
    "|     5 | **Only in Lasso**                | 60                                    | Features selected **only by Lasso**. These had small but non-zero coefficients after regularization, not meeting the threshold or being selected by Forward Stepwise. They may have weak or noise-driven impact. |\n",
    "|     6 | **Only in Forward Stepwise**     | 12                                    | Features selected **only by Forward Stepwise**. These were added incrementally because they improved model performance, even if their individual effects were weak. Forward Stepwise is sensitive to such gains. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e11c2",
   "metadata": {},
   "source": [
    "The feature selection comparison revealed that **9 features** were consistently selected by **all three methods**—Lasso, Thresholding, and Forward Stepwise. These features are considered the most robust and reliable predictors, as they were repeatedly identified as important across different selection strategies.\n",
    "\n",
    "The intersection between **Lasso and Thresholding** yielded **24 shared features**, with **no features selected exclusively by Thresholding**. This is expected, since Thresholding is applied on top of Lasso and can only **filter out** features, not add new ones. Therefore, these 24 features represent the **subset of Lasso-selected variables** that passed the coefficient threshold and are **deemed important for prediction** based on their influence on the response variable.\n",
    "\n",
    "A substantial overlap of **52 features** was observed between **Lasso and Forward Stepwise**, indicating strong agreement between these two methods despite their different selection mechanisms. However, not all of these features were retained after Thresholding, which suggests that **using all 52 may introduce complexity or overfitting**, as some have relatively small contributions.\n",
    "\n",
    "Finally, the features shared between **Thresholding and Forward Stepwise** are identical to those selected by all three methods, further emphasizing their stability and central importance in building a robust predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d33e2f",
   "metadata": {},
   "source": [
    "### 3.4. Tree-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dea000",
   "metadata": {},
   "source": [
    "#### 3.4.1. Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c296452",
   "metadata": {},
   "source": [
    "##### 3.4.1.1. Fitting and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f01500",
   "metadata": {},
   "source": [
    "A regression tree is fitted to the training data using the MSE as the splitting criterion. All input features are considered, and splits are performed only at nodes with more than 10 samples. The resulting tree structure is then visualized using the graphviz package, which generates a graphical representation of all nodes and their corresponding decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef16ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\" # Adjust this path as needed for thw macOS users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a regression tree on the training data using the mse metric for splitting, considering all the features and splitting if there are more than 10 samples at a node.\n",
    "tree = DecisionTreeRegressor(criterion='squared_error', max_features=None, min_samples_split=20)\n",
    "\n",
    "tree_est = tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the tree with the help of the graphviz and the iPython package\n",
    "# use sklearn's export to generate the dot-data string file with all the nodes and their props.\n",
    "dot_data = export_graphviz(tree_est, out_file='boston_tree.dot',feature_names=concrete_df.columns[0:-1],filled=True, \n",
    "                           rounded=True, special_characters=True)\n",
    "\n",
    "with open('boston_tree.dot') as f:\n",
    "    dot_graph = f.read()  \n",
    "\n",
    "# create the source object\n",
    "I = graphviz.Source(dot_graph, format='png', engine='dot')\n",
    "# Use ipython Image to shrink the rendered image of the source obj to fit into jupyter nb.\n",
    "Image(I.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b97c03",
   "metadata": {},
   "source": [
    "##### 3.4.1.2. Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fc8c8",
   "metadata": {},
   "source": [
    "The trained decision tree model is used to generate predictions on the test set. Model performance is then evaluated by calculating the R² score and the MSE on the test data, providing insight into both the explained variance and prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e12493",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree_est.predict(X_test)\n",
    "\n",
    "print(\"R² Score on Test Set:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cdde8",
   "metadata": {},
   "source": [
    "##### 3.4.1.3 Feature Importance Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d413f",
   "metadata": {},
   "source": [
    "To further interpret the decision tree model, feature importance scores were computed to quantify each variable’s contribution to the prediction task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(data=tree.feature_importances_, index=list(concrete_df.columns[0:-1]))\n",
    "feature_importances.sort_values(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b93f05",
   "metadata": {},
   "source": [
    "| Feature                                                   | Importance | Meaning                                                                                                                                             |\n",
    "| --------------------------------------------------------- | ---------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Age (day)**                                             | 0.355     | The age of concrete is the most important feature, contributing \\~35,5% of the total split importance. It heavily influences the strength prediction. |\n",
    "| **Cement (component 1)(kg in a m^3 mixture)**             | 0.305     | Cement amount is the second most influential feature, about 30,5% importance — a key material affecting strength.                                     |\n",
    "| **Water (component 4)(kg in a m^3 mixture)**              | 0.110     | Water content also plays a significant role (\\~11%), impacting the mixture's properties.                                                            |\n",
    "| **Superplasticizer (component 5)(kg in a m^3 mixture)**   | 0.095     | Superplasticizer's role is noticeable (\\~9.5%), affecting workability and strength.                                                                 |\n",
    "| **Blast Furnace Slag (component 2)(kg in a m^3 mixture)** | 0.054     | Moderately important (\\~5.4%), influencing the final properties.                                                                                    |\n",
    "| **Fine Aggregate (component 7)(kg in a m^3 mixture)**     | 0.039     | Low importance (\\~3.9%), minor influence on predictions.                                                                                            |\n",
    "| **Fly Ash (component 3)(kg in a m^3 mixture)**            | 0.030     | Minimal contribution (\\~3%).                                                                                                                      |\n",
    "| **Coarse Aggregate (component 6)(kg in a m^3 mixture)**   | 0.013     | Smallest importance (\\~1.3%), less relevant in this model.                                                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19653ff6",
   "metadata": {},
   "source": [
    " The resulting table lists the features in descending order of importance and provides a brief interpretation of each variable’s role. Higher importance values indicate a greater influence on the model’s predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa31c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = tree_est.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.yticks(range(len(importances)), feature_names[indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c7c3f",
   "metadata": {},
   "source": [
    "In addition to the table, the feature importances are visualized in the bar chart above. This graphical representation provides a more intuitive comparison of the relative influence of each variable, clearly highlighting the dominant role of `Age` and `Cement` in the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d18ed1",
   "metadata": {},
   "source": [
    "##### 3.4.1.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced01293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15, None],\n",
    "    'min_samples_split': [2, 10, 20, 50],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2'] \n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree,\n",
    "    param_grid=param_grid,\n",
    "    cv=kf,                # 10-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE as scoring metric\n",
    "    n_jobs=-1,           # Use all CPU cores\n",
    "    verbose=1            # Print progress\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator for prediction\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_tree.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse_hyper = mean_squared_error(y_test, y_pred)\n",
    "r2_hyper = r2_score(y_test, y_pred)\n",
    "print(f\"Test set MSE: {mse_hyper:.4f}\")\n",
    "print(f\"Test set R²: {r2_hyper:.4f}\")\n",
    "\n",
    "y_train_pred = best_tree.predict(X_train)\n",
    "train_mse_hyper = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2_hyper = r2_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train the optimized tree\n",
    "optimized_tree = DecisionTreeRegressor(\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    random_state=0\n",
    ")\n",
    "optimized_tree.fit(X_train, y_train)\n",
    "\n",
    "# 2. Export the tree to DOT format\n",
    "dot_data = export_graphviz(\n",
    "    optimized_tree,\n",
    "    out_file=None,\n",
    "    feature_names=concrete_df.columns[:-1],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True\n",
    ")\n",
    "\n",
    "# 3. Render and display using graphviz\n",
    "graph = graphviz.Source(dot_data, format='png')\n",
    "Image(graph.render('optimized_tree'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3523b",
   "metadata": {},
   "source": [
    "Pruning (Cost-Complexity pruning): select a subtree leading to the lowest test error rate.\n",
    "\n",
    "Motivation: \n",
    "- ccp_alpha is a regularization hyperparameter, increasing it prunes more, reducing overfitting.\n",
    "- We have already got a strong performance (R² ≈ 0.8055), and pruning it can imporve generalization further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a full tree\n",
    "path = DecisionTreeRegressor(random_state=0).cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Optionally reduce the number of alphas (too many can slow down grid search)\n",
    "ccp_alphas = np.unique(np.round(ccp_alphas, 4))  # Remove tiny variations\n",
    "ccp_alphas = ccp_alphas[::5]  # Sample every 5th alpha if too many\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdff134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'ccp_alpha': ccp_alphas  # ← added\n",
    "}\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "grid = GridSearchCV(tree, param_grid, cv=kf, scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate and visualize\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "best_tree = grid.best_estimator_\n",
    "y_pred = best_tree.predict(X_test)\n",
    "\n",
    "print(\"Test set MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Test set R²:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af295b",
   "metadata": {},
   "source": [
    "Interpretation of Final Model Results:\n",
    "- Best Hyperparameters (with pruning):\n",
    "    - ccp_alpha = 0.001\n",
    "    - max_depth = None\n",
    "    - max_features: None\n",
    "    - min_samples_leaf = 1\n",
    "    - min_samples_spilit = 2\n",
    "- Test Set Performance:\n",
    "    - MSE: ≈ 57.83, which is still very low\n",
    "    - R²: ≈ 0.78, it is still very solid fit (explains ~78% of variance)\n",
    "- Pruning Effect:\n",
    "    - The ccp_alpha = 0.001 suggests aggressive pruning helped generalize better.\n",
    "    - The earlier R² was ≈ 0.7813 without pruning, so pruning leads to a higher R² and a **simpler, more generalizable tree**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90ea4f",
   "metadata": {},
   "source": [
    "pruned tree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3697aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit the optimized and pruned tree\n",
    "final_tree = DecisionTreeRegressor(\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    ccp_alpha=0.2008,\n",
    "    random_state=0\n",
    ")\n",
    "final_tree.fit(X_train, y_train)\n",
    "\n",
    "# 2. Export to DOT format\n",
    "dot_data = export_graphviz(\n",
    "    final_tree,\n",
    "    out_file=None,\n",
    "    feature_names=X.columns if hasattr(X, 'columns') else [f\"X{i}\" for i in range(X.shape[1])],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True\n",
    ")\n",
    "\n",
    "# 3. Render with graphviz\n",
    "tree_graph = graphviz.Source(dot_data, format=\"png\")\n",
    "tree_graph.render(\"final_pruned_tree\", cleanup=True)\n",
    "Image(\"final_pruned_tree.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded31776",
   "metadata": {},
   "source": [
    "##### 3.4.1.5 Model Evaluation & Diagonistic Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e91f8",
   "metadata": {},
   "source": [
    "Residual Analysis (difference between actual and predicted values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, edgecolor='k', facecolor='none')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec8302",
   "metadata": {},
   "source": [
    "Parity Plot (checks how close predictions are to actual values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48087b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, edgecolor='k', facecolor='none')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Ideal line\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a04c4",
   "metadata": {},
   "source": [
    "Model Complexity Check - check if the tree is too deep or has too many leaves. A very large tree usually overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813caf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Tree depth:\", final_tree.get_depth())\n",
    "print(\"Number of leaves:\", final_tree.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c12a4",
   "metadata": {},
   "source": [
    "check Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred = final_tree.predict(X_train)\n",
    "y_test_pred = final_tree.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2d7bf",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "The model fits the training data very well but is less accurate on new data, indicating some degree of overfitting, but not severe.\n",
    "\n",
    "The gap between train and test metrics has reduced compared to the previous attempts, so this model generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e7506",
   "metadata": {},
   "source": [
    "| Model # | Model Name                        | Test R² | Test MSE |\n",
    "|---------|----------------------------------|---------|----------|\n",
    "| 1       | Decision Tree - Basic             | 0.7656  | 61.7348  |\n",
    "| 2       | Decision Tree - Hyperparameter Tuned | **0.7813**  | **57.6107**  |\n",
    "| 3       | Decision Tree - With Pruning      | 0.7804  | 57.8274  |\n",
    "| 4       | Decision Tree - Final Pruned      | 0.7794  | 58.1049  |\n",
    "\n",
    "\n",
    "**Note:** Model 2 shows the best overall performance with the highest Test R² and lowest Test MSE. We plan to append this model’s results to `model_results_df` for further comparison with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Decision Tree - Hyperparameter Tuned\"\n",
    "\n",
    "# append the result to Dataframe\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    train_r2_hyper,\n",
    "    train_mse_hyper,\n",
    "    r2_hyper,\n",
    "    mse_hyper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccc0cc",
   "metadata": {},
   "source": [
    "#### 3.4.2. Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081abb3f",
   "metadata": {},
   "source": [
    "##### 3.4.2.1. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7095c8",
   "metadata": {},
   "source": [
    "To determine an efficient ensemble size for the Random Forest model, different values of `n_estimators` were evaluated using the out-of-bag (OOB) score — a built-in validation method where each tree is evaluated on data not used during its own training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [50, 100, 200, 300, 500]:\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=0, oob_score=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{n} trees - OOB Score: {model.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f6598",
   "metadata": {},
   "source": [
    "Based on the results, the OOB score obtained with 200 trees was the highest among all tested values, suggesting that this configuration offers a favorable trade-off between performance and computational efficiency. Therefore, a bagged ensemble model was constructed using 200 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a009109",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagger = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    criterion='squared_error',         \n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "bag_est = bagger.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e6dc4",
   "metadata": {},
   "source": [
    "To assess the predictive performance of the final bagged ensemble model, the test set R² and MSE are first computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the test MSE\n",
    "y_pred = bag_est.predict(X_test)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test R² = {test_r2:.4f}\")\n",
    "print(f\"Test MSE = {test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42111771",
   "metadata": {},
   "source": [
    "With a high R² value of 0.9246 and a low test MSE of 19.8617, the model demonstrates strong generalization capabilities and accurate predictions on unseen data. To further support this evaluation, a scatter plot is generated comparing predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the test MSE\n",
    "# Plot the predicted vs the actual medv response\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "ax.scatter(y_pred, y_test, facecolor='None', edgecolor='b')\n",
    "# add a reference unity line\n",
    "ax.plot([min(y_pred), max(y_pred)], [min(y_test), max(y_test)], linestyle='--', color='k');\n",
    "ax.set_xlabel('y_predicted')\n",
    "ax.set_ylabel('y_actual')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e5caaf",
   "metadata": {},
   "source": [
    "Most points lie close to the diagonal reference line, visually confirming the model's strong predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37bfd6",
   "metadata": {},
   "source": [
    "##### 3.4.2.2 Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba670608",
   "metadata": {},
   "source": [
    "To further evaluate the model performance, a random forest regressor with 200 trees was trained using the same training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550abe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random forest and fit it to the training data\n",
    "forest = RandomForestRegressor(n_estimators=200, criterion='squared_error', max_features=4, bootstrap=True, \n",
    "                               oob_score=True, random_state=0 )\n",
    "\n",
    "forest_est = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97282fef",
   "metadata": {},
   "source": [
    "Subsequently, the test R² and test MSE are computed to evaluate the model’s generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test MSE of the random forest\n",
    "y_train_pred = forest_est.predict(X_train)\n",
    "train_mse_basic = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2_basic = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = forest_est.predict(X_test)\n",
    "test_mse_basic = mean_squared_error(y_test, y_pred)\n",
    "test_r2_basic = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test R² = {test_r2_basic:.4f}\")\n",
    "print(f\"Test MSE = {test_mse_basic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296b0a4",
   "metadata": {},
   "source": [
    "##### 3.4.2.3. Comparison of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e9ded",
   "metadata": {},
   "source": [
    "Interpretation: \n",
    "\n",
    "| Model         | Description                              | Test MSE    |\n",
    "| ------------- | ---------------------------------------- | ----------- |\n",
    "| Bagging       | Uses all features, averages many trees   | 19.97       |\n",
    "| Random Forest | Random subset of features for each split | **19.89**  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed200c4",
   "metadata": {},
   "source": [
    "Random Forest introduces feature randomness (via max_features) to decorrelate trees more than standard bagging. So in this case, both the test MSE and test R² show slight improvements compared to Bagging, indicating that Random Forest offers a marginal but consistent performance advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652c7a7",
   "metadata": {},
   "source": [
    "##### 3.4.2.4. Model Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a1320",
   "metadata": {},
   "source": [
    "**Feature importance values** derived from the trained random forest model are computed and sorted to quantify the relative contribution of each input variable to the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the feature importances\n",
    "feature_importances = pd.Series(data=forest_est.feature_importances_, index=list(concrete_df.columns[0:-1]))\n",
    "feature_importances = feature_importances.sort_values(axis=0, ascending=False)\n",
    "feature_importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c87935",
   "metadata": {},
   "source": [
    "To enhance interpretability, the results are visualized in a horizontal bar chart, offering a clear comparison of each variable’s predictive influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabd356",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(importances)), importances[indices][::-1], align='center')\n",
    "plt.yticks(range(len(importances)), np.array(feature_names)[indices][::-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eec63a",
   "metadata": {},
   "source": [
    "After fitting the Random Forest model, the feature importance analysis reveals which input variables most strongly influence the prediction. The feature `Age` is the most important, indicating it has the greatest impact on the concrete strength. `Cement content` follows as the second most influential feature. Other components like `Water`, `Superplasticizer`, and `Blast Furnace Slag` contribute moderately, while features such as `Fly Ash` have relatively low importance.\n",
    "\n",
    "This insight helps to understand the key factors driving the model’s predictions, supports domain knowledge validation, and can guide feature selection or further data collection efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# List of all feature names in order, matching X_train columns\n",
    "feature_names = [\n",
    "    'Cement (component 1)(kg in a m^3 mixture)', \n",
    "    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)', \n",
    "    'Fly Ash (component 3)(kg in a m^3 mixture)', \n",
    "    'Water  (component 4)(kg in a m^3 mixture)', \n",
    "    'Superplasticizer (component 5)(kg in a m^3 mixture)', \n",
    "    'Coarse Aggregate  (component 6)(kg in a m^3 mixture)', \n",
    "    'Fine Aggregate (component 7)(kg in a m^3 mixture)', \n",
    "    'Age (day)'\n",
    "]\n",
    "\n",
    "# Top features to keep\n",
    "top_features = [\n",
    "    'Age (day)', \n",
    "    'Cement (component 1)(kg in a m^3 mixture)', \n",
    "    'Water  (component 4)(kg in a m^3 mixture)', \n",
    "    'Superplasticizer (component 5)(kg in a m^3 mixture)', \n",
    "    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)'\n",
    "]\n",
    "\n",
    "# Get their column indices\n",
    "top_indices = [feature_names.index(f) for f in top_features]\n",
    "\n",
    "# Select columns by index\n",
    "X_train_reduced = X_train[:, top_indices]\n",
    "X_test_reduced = X_test[:, top_indices]\n",
    "\n",
    "# Train the model on reduced features\n",
    "model_reduced = RandomForestRegressor(random_state=42)\n",
    "model_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_reduced = model_reduced.predict(X_test_reduced)\n",
    "print(\"Test R²:\", r2_score(y_test, y_pred_reduced))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_pred_reduced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc098cab",
   "metadata": {},
   "source": [
    "To enhance model efficiency, the five most important features — identified via feature importance analysis — are selected. A Random Forest is then trained on this reduced feature set, yielding comparable performance, which suggests that excluding less relevant features can maintain accuracy while reducing complexity and noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a34f2",
   "metadata": {},
   "source": [
    "##### 3.4.2.5. Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b5258a",
   "metadata": {},
   "source": [
    "This section performs hyperparameter tuning for the random forest model using `GridSearchCV`, identifying the optimal combination of `max_depth` and `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Random Forest using GridSearchCV\n",
    "# Define the model\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, None],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=kf, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit to training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf.predict(X_test)\n",
    "print(\"Test R²:\", r2_score(y_test, y_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8ae4b",
   "metadata": {},
   "source": [
    "**Test R²** improved slightly to ~0.9228 (from 0.9204 before)\n",
    "\n",
    "**Test MSE** decreased to ~20.3337 (from 20.95 before)\n",
    "\n",
    "This means the model’s performance got a bit better — more accurate and slightly more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf6e3d",
   "metadata": {},
   "source": [
    "##### 3.4.2.6. Model evaluation and diagnostic checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd9f29",
   "metadata": {},
   "source": [
    "To achieve model evaluation and diagnostic checking, **residual analysis** is first conducted. This involves computing the residuals by subtracting predicted values from the true values, and then visualizing them against the predicted values in a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual analysis\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='k')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals (True - Predicted)')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16789250",
   "metadata": {},
   "source": [
    "This residual plot shows the difference between the true and predicted values (residuals) across the range of predicted values.\n",
    "\n",
    "- The red dashed line represents the ideal case where residuals = 0.\n",
    "- Most points are randomly scattered around this line, indicating that the model errors are approximately unbiased.\n",
    "- There is no apparent curve or funnel-shaped pattern, which suggests:\n",
    "  - No major non-linearity in the data.\n",
    "  - No evidence of heteroscedasticity (i.e., constant variance of residuals).\n",
    "- A few outliers (e.g., residuals below -15 and above 15) may indicate potential extreme cases or noisy data points, which can be further investigated.\n",
    "\n",
    "**Conclusion**: The residuals appear to be randomly and symmetrically distributed around zero, supporting the assumption of linearity and homoscedasticity in the model. This implies that the model is reasonably well-fitted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246322f",
   "metadata": {},
   "source": [
    "To further assess the model’s generalization capability, the **OOB-score** is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfa0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train model with OOB enabled\n",
    "rf_oob = RandomForestRegressor(n_estimators=200, max_depth=best_rf.max_depth,\n",
    "                               max_features=best_rf.max_features,\n",
    "                               random_state=0, oob_score=True)\n",
    "rf_oob.fit(X_train, y_train)\n",
    "\n",
    "print(f\"OOB R² score: {rf_oob.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc660c",
   "metadata": {},
   "source": [
    "OOB score being close to the test R² (~0.92) confirms the train-test split results are reliable and not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5da8a",
   "metadata": {},
   "source": [
    "| Model # | Model Name                  | Test R²   | Test MSE   |\n",
    "|---------|-----------------------------|-----------|------------|\n",
    "| 1       | RF Basic (n_estimators=200) | **0.9245** | **19.8936** |\n",
    "| 2       | RF Reduced Features (Top 5) | 0.9204    | 20.9593    |\n",
    "| 3       | RF with GridSearchCV Tuning | 0.9228    | 20.3337    |\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "- The basic Random Forest model (Model 1) with 200 trees surprisingly outperforms the tuned and reduced-feature models.\n",
    "- Possible reasons include:\n",
    "  - **Sufficient complexity:** The basic model already captures the underlying patterns well without overfitting.\n",
    "  - **Feature reduction may lose useful information:** Removing features (Model 2) reduces the input space, potentially discarding relevant data, leading to slightly worse performance.\n",
    "  - **GridSearchCV tuning might not explore the optimal parameter space fully:** The parameter grid may be limited or the best parameters may not generalize better than the default ones.\n",
    "  - **Random variation:** Due to randomness in training and limited test data, small differences in metrics are expected.\n",
    "- In practice, a simpler default model can perform well if the data is clean and the signal strong. Further tuning with broader parameter search or ensemble methods might improve results beyond the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8351d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append basic RF model to do further Model Comparison\n",
    "model_name = \"Random Forest Basic\"\n",
    "\n",
    "# append the result to Dataframe\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    train_r2_basic,\n",
    "    train_mse_basic,\n",
    "    test_r2_basic,\n",
    "    test_mse_basic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bc730",
   "metadata": {},
   "source": [
    "#### 3.4.3. Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441ace2",
   "metadata": {},
   "source": [
    "##### 3.4.3.1. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1775d5",
   "metadata": {},
   "source": [
    "A Gradient Boosting Regressor is instantiated with specified hyperparameters and then trained on the full training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and training a Gradient Boosting Regressor model on the traning data\n",
    "booster = GradientBoostingRegressor(\n",
    "    loss='squared_error',        \n",
    "    learning_rate=0.01,          \n",
    "    n_estimators=200,            \n",
    "    max_depth=3,                 \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "boost_est = booster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fceaf7",
   "metadata": {},
   "source": [
    "The trained model is used to predict target values on the test set, and its performance is assessed using MSE and R² score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31726856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set and evaluate model performance\n",
    "y_pred = boost_est.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9221b",
   "metadata": {},
   "source": [
    "##### 3.4.3.2. Feature importance plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a4740",
   "metadata": {},
   "source": [
    "Feature importances from the Gradient Boosting model are extracted and sorted in descending order to identify the most influential predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(data=boost_est.feature_importances_, index=list(concrete_df.columns[:-1]))\n",
    "sorted_feature_importances = feature_importances.sort_values(ascending=False)\n",
    "sorted_feature_importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e4b96",
   "metadata": {},
   "source": [
    "The Gradient Boosting model identifies **\"Age\"** and **\"Cement\"** as the most influential factors in predicting the target variable, indicating these features have the strongest impact on the model’s predictions. Other features like **\"Superplasticizer\"** and **\"Water\"** also contribute notably but to a lesser degree. Features such as **\"Coarse Aggregate\"** and **\"Fly Ash\"** show minimal influence, suggesting they play a minor role in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560d272",
   "metadata": {},
   "source": [
    "##### 3.4.3.3. Partial dependence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efffc75",
   "metadata": {},
   "source": [
    "Partial dependence plots for the three most important features — Superplasticizer, Cement, and Age — are presented to provide insight into how changes in each individual variable affect the model’s predicted outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idxs = np.argsort(feature_importances.values)[-3:]\n",
    "\n",
    "# Plot PDP\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    boost_est,                  \n",
    "    X_train,                    \n",
    "    features=feature_idxs,      \n",
    "    feature_names=feature_importances.index.tolist(),\n",
    "    ax=ax\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b71289",
   "metadata": {},
   "source": [
    "There might be an optimal range for **superplasticizer** where its positive effect on the predicted outcome is most pronounced, and further increases might not be beneficial. For **Cement**, the \"steps\" could suggest that the model has specific \"regimes\" or thresholds for cement content where the properties change significantly. Higher cement content generally leads to a better (higher) predicted outcome, but these improvements might not be linear. The plot of **Age**  implies that the most significant improvements in the predicted outcome due to aging occur within the first couple of months. Beyond this initial period, further aging has a negligible additional impact on the predicted outcome. This could indicate a saturation point where the material's properties stabilize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d1ea3",
   "metadata": {},
   "source": [
    "##### 3.4.3.4. Model refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ea3d5",
   "metadata": {},
   "source": [
    "As a first step, **feature selection** is performed by retaining only the most important features based on the **feature importances** derived from the gradient boosting model. Subsequently, the model is re-trained using only the selected features, and the test MSE is computed to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = [\n",
    "    'Age',\n",
    "    'Cement',\n",
    "    'Superplasticizer',\n",
    "    'Water',\n",
    "    'Blast Furnace Slag'\n",
    "]\n",
    "\n",
    "X_selected = concrete_df[important_features]\n",
    "y = concrete_df['Concrete compressive strength'] \n",
    "\n",
    "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "booster_sel = GradientBoostingRegressor(\n",
    "    loss='squared_error',       \n",
    "    learning_rate=0.001,\n",
    "    n_estimators=5000,\n",
    "    max_depth=4,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "boost_est_sel = booster_sel.fit(X_train_sel, y_train_sel)\n",
    "\n",
    "y_pred_sel = boost_est_sel.predict(X_test_sel)\n",
    "mse_sel = mean_squared_error(y_test_sel, y_pred_sel)\n",
    "r2_sel = r2_score(y_test_sel, y_pred_sel)\n",
    "print(\"Test MSE with selected features:\", mse_sel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1e4a2",
   "metadata": {},
   "source": [
    "In the second step, **feature selection** is refined by focusing exclusively on the three variables with the most distinct patterns in the partial dependence plots. The resulting test MSE is subsequently calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features strongly supported by PDP interpretation\n",
    "important_features_pdp = [\n",
    "    'Age',\n",
    "    'Cement',\n",
    "    'Superplasticizer'\n",
    "]\n",
    "\n",
    "X_selected_pdp = concrete_df[important_features_pdp]\n",
    "y = concrete_df['Concrete compressive strength']\n",
    "\n",
    "\n",
    "# Set up the gradient boosting regressor with early stopping\n",
    "booster_sel = GradientBoostingRegressor(\n",
    "    loss='squared_error',\n",
    "    learning_rate=0.01,      \n",
    "    n_estimators=1000,       \n",
    "    max_depth=4,\n",
    "    random_state=0,\n",
    "    validation_fraction=0.1, \n",
    "    n_iter_no_change=50,  \n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "# Fit model with early stopping\n",
    "boost_est_sel = booster_sel.fit(X_train_sel, y_train_sel)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_sel = boost_est_sel.predict(X_test_sel)\n",
    "mse_sel = mean_squared_error(y_test_sel, y_pred_sel)\n",
    "r2_sel = r2_score(y_test_sel, y_pred_sel)\n",
    "\n",
    "print(\"Test MSE with PDP-based selected features:\", mse_sel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edd40f",
   "metadata": {},
   "source": [
    "The dropped features might still contain some useful information, even if their importance seemed low, as indicated by the slightly better performance (lower test MSE) when using all selected features (**27.25**) compared to using only the PDP-based subset (**27.12**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7246cb3",
   "metadata": {},
   "source": [
    "##### 3.4.3.5. Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375a892",
   "metadata": {},
   "source": [
    "To further improve the model, hyperparameter tuning is conducted via `RandomizedSearchCV` on the `GradientBoostingRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb82ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "gbr = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,           # number of parameter settings sampled\n",
    "    cv=kf,                # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # minimize MSE\n",
    "    n_jobs=-1,           # use all cores\n",
    "    random_state=0,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_booster = random_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_booster.predict(X_test)\n",
    "\n",
    "# Calculate MSE and R2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test R2 score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68bd872",
   "metadata": {},
   "source": [
    "The Gradient Boosting model uses carefully chosen hyperparameters to balance accuracy and generalization. It builds 500 trees with a maximum depth of 4, limiting complexity to avoid overfitting. Each tree is trained on 60% of the data and considers only a subset of features at each split, adding randomness to improve robustness. Minimum samples per split and leaf prevent overly specific rules, while a learning rate of 0.1 ensures gradual, stable learning. Overall, these settings help the model perform well on unseen data while avoiding overfitting. The tuned Gradient Boosting model shows strong predictive performance, achieving a low test MSE of 17.3755 and a high R² of 0.934."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4385949",
   "metadata": {},
   "source": [
    "##### 3.4.3.6. Model evaluation and diagnostic checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0597669",
   "metadata": {},
   "source": [
    "To evaluate the model and perform diagnostic checking, **residual analysis** is first conducted to examine whether the errors are randomly distributed and free of systematic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbe7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_pred, residuals, edgecolor='k', facecolor='none')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90485c45",
   "metadata": {},
   "source": [
    "The residual plot shows that the residuals are evenly scattered around zero without any clear pattern, indicating that the model's errors are randomly distributed. The consistent spread of residuals suggests that the assumption of constant variance is met, and although there are a few isolated outliers, they do not undermine the overall fit. This implies that the model captures the data well, with no strong bias or missed nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, edgecolor='k', facecolor='none')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e4831",
   "metadata": {},
   "source": [
    "Actual vs Predicted Plot Interpretation\n",
    "\n",
    "This scatter plot shows the relationship between the actual values (x-axis) and the predicted values (y-axis) on the test set.\n",
    "\n",
    "- The red dashed line represents the ideal prediction line where `Predicted = Actual`.\n",
    "- Most points lie very close to this line, indicating that the model is performing well and making accurate predictions.\n",
    "- A few outliers can be observed, especially at higher value ranges, suggesting that the model may slightly struggle with extreme values.\n",
    "- There is no apparent systematic bias (e.g., consistently over- or under-predicting).\n",
    "\n",
    "**Conclusion**: The model demonstrates strong predictive power with high alignment between actual and predicted values. This plot supports the reliability of the model in generalizing to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509256d",
   "metadata": {},
   "source": [
    "A comparison between **training and testing performance** is then performed by calculating the R² and MSE for both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042435d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred = best_booster.predict(X_train)\n",
    "y_test_pred = best_booster.predict(X_test)\n",
    "\n",
    "# R²\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# MSE\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train R²: {r2_train:.3f}, Test R²: {r2_test:.3f}\")\n",
    "print(f\"Train MSE: {mse_train:.3f}, Test RMSE: {mse_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0d584",
   "metadata": {},
   "source": [
    "The gradient boosting model shows excellent fit on training data and still performs very well on test data. The performance drop is moderate and expected in real-world modeling. There is no strong evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc87c70",
   "metadata": {},
   "source": [
    "Subsequently, a comparative summary of different GBR models is provided below, highlighting their respective Test R² and MSE values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7fa57",
   "metadata": {},
   "source": [
    "| Model # | Model Name                   | Test R² | Test MSE |\n",
    "|---------|------------------------------|---------|----------|\n",
    "| 1   | GBR Basic                    | 0.7764  | 58.8864  |\n",
    "| 2   | GBR with Top-5 Features      | 0.8965  | 27.2485  |\n",
    "| 3   | GBR with PDP-based Features  | 0.8970  | 27.1193  |\n",
    "| 4   | GBR Best booster             | **0.9340**  | **17.3755**  |\n",
    "\n",
    "Based on the comparison table, **Model 4 (GBR Best booster)** achieved the **highest R² score (0.9340)** and the **lowest Test MSE (17.3755)** among all tested GBR variants. This indicates that it offers the best generalization performance and predictive accuracy on unseen data.\n",
    "\n",
    "Given its superior performance, we selected **GBR Best booster** as the final Gradient Boosting model and included it in `model_results_df` for a comprehensive comparison against other models in our project. This ensures a consistent and fair evaluation across all candidate algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6666265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Gradient Boosting - Best Booster\"\n",
    "\n",
    "# append the result to Dataframe\n",
    "model_results_df = record_model_results(\n",
    "    model_results_df,\n",
    "    model_name,\n",
    "    r2_train,\n",
    "    mse_train,\n",
    "    r2_test,\n",
    "    mse_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753e25b",
   "metadata": {},
   "source": [
    "## 4. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort model results by Test R²\n",
    "model_results_df = model_results_df.sort_values(by=\"Test_R2\", ascending=False)\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38129d1",
   "metadata": {},
   "source": [
    "### Summary Table (Ranked by Test R²)\n",
    "\n",
    "| Model                                | Test R²  | Test MSE | R² Gap   |\n",
    "|--------------------------------------|----------|----------|----------|\n",
    "| Gradient Boosting - Best Booster     | 0.934029 | 17.375   | 0.058194 |\n",
    "| Random Forest Basic                  | 0.924468 | 19.894   | 0.060550 |\n",
    "| Lasso (alpha=0.0102)                 | 0.840162 | 42.098   | 0.083772 |\n",
    "| Ridge (alpha=0.9103)                 | 0.828407 | 45.194   | 0.099844 |\n",
    "| Polynomial Regression (degree: 3)    | 0.805898 | 51.122   | 0.126220 |\n",
    "| Decision Tree - Hyperparameter Tuned| 0.781263 | 57.611   | 0.214356 |\n",
    "| Basic Linear Regression              | 0.636961 | 95.617   | -0.027837|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "# create ShortName by extracting a simplified model name from the 'Model' column\n",
    "if 'ShortName' not in model_results_df.columns:\n",
    "    model_results_df['ShortName'] = model_results_df['Model'].apply(\n",
    "        lambda x: x.split('-')[0].strip() if '-' in x else x.split('(')[0].strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605358e",
   "metadata": {},
   "source": [
    "### Model Performance Comparison: Test R² and Normalized Test MSE\n",
    "\n",
    "The plot below compares multiple regression models based on two key performance metrics:\n",
    "\n",
    "- **Test R² (Coefficient of Determination):** This measures how well the model's predictions match the actual data. A higher R² (closer to 1) means the model explains more of the variance in the target variable, which is better.\n",
    "\n",
    "- **Normalized Test MSE (Mean Squared Error):** This measures the average squared difference between the predicted and actual values. A lower MSE means better predictive accuracy. Here, to allow easy comparison alongside R², the MSE values are normalized by dividing each model’s Test MSE by the highest MSE among all models. This scales the MSE scores between 0 and 1, where 1 corresponds to the worst model's MSE.\n",
    "\n",
    "#### How to Read the Plot\n",
    "\n",
    "- Each model is listed on the vertical axis.\n",
    "- For each model, there are two horizontal bars:\n",
    "  - **Blue bar:** Represents the Test R² score. Longer bars mean higher R², thus better performance.\n",
    "  - **Orange bar:** Represents the normalized Test MSE. Since MSE is an error metric, shorter bars mean lower error and better performance.\n",
    "\n",
    "#### What Indicates a Good Model?\n",
    "\n",
    "- A good model will have a **long blue bar** (high Test R²) and a **short orange bar** (low normalized Test MSE).\n",
    "- If a model has both bars near the top (blue close to 1, orange close to 0), it suggests the model predicts the target variable very accurately.\n",
    "\n",
    "#### Why Normalize MSE?\n",
    "\n",
    "- The raw MSE values can vary widely, making it difficult to compare visually on the same scale as R².\n",
    "- Normalizing by the worst (maximum) MSE compresses all MSE values between 0 and 1.\n",
    "- This allows us to place Test R² and normalized Test MSE side-by-side for easier visual comparison.\n",
    "\n",
    "This combined visualization helps quickly identify which models have the best balance of explained variance and prediction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20af349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize MSE\n",
    "max_mse = model_results_df[\"Test_MSE\"].max()\n",
    "model_results_df[\"Test_MSE_norm\"] = model_results_df[\"Test_MSE\"] / max_mse\n",
    "\n",
    "# Sort models by Test R² score for consistent bar order\n",
    "sorted_df = model_results_df.sort_values(\"Test_R2\", ascending=True)  \n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_height = 0.4\n",
    "indices = np.arange(len(sorted_df))\n",
    "\n",
    "plt.barh(indices, sorted_df[\"Test_R2\"], bar_height, label=\"Test R²\", color=\"royalblue\")\n",
    "\n",
    "plt.barh(indices + bar_height, sorted_df[\"Test_MSE_norm\"], bar_height, label=\"Normalized Test MSE\", color=\"orange\")\n",
    "\n",
    "plt.yticks(indices + bar_height / 2, sorted_df[\"Model\"])\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Comparison of Test R² and Normalized Test MSE\")\n",
    "plt.xlim(0, 1.1)\n",
    "plt.xlabel(\"Score\")\n",
    "\n",
    "# Add value labels next to each bar\n",
    "for i in range(len(sorted_df)):\n",
    "    plt.text(sorted_df[\"Test_R2\"].iloc[i] + 0.02, i, f\"{sorted_df['Test_R2'].iloc[i]:.3f}\", va='center', fontsize=9)\n",
    "    plt.text(sorted_df[\"Test_MSE_norm\"].iloc[i] + 0.02, i + bar_height, f\"{sorted_df['Test_MSE_norm'].iloc[i]:.3f}\", va='center', fontsize=9)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f272aa89",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Among all the tested models, the **Gradient Boosting Regressor (Best Booster)** stands out as the optimal model. It achieves the **highest Test R²**, indicating the best fit and strongest explanatory power on the test data, and the **lowest normalized Test MSE**, reflecting superior predictive accuracy with minimal error.\n",
    "\n",
    "This combination of the highest R² and the lowest normalized MSE confirms that the Best Booster model provides the most reliable and accurate predictions for our concrete compressive strength dataset. Therefore, it is the preferred choice for further analysis and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2841a4f",
   "metadata": {},
   "source": [
    "### How to Read the Overfitting vs Generalization Scatter Plot\n",
    "\n",
    "This scatter plot visualizes the relationship between training and testing performance for different models, helping to assess their fit quality and generalization ability.\n",
    "\n",
    "- **X-axis (Training R²):** Represents how well each model fits the training data. Higher values mean better fit on the training set.\n",
    "- **Y-axis (Testing R²):** Represents how well the model generalizes to unseen data. Higher values mean better predictive performance on the test set.\n",
    "- **Point Size (R² Gap):** The size of each point corresponds to the gap between training and testing R² scores. Larger points indicate a bigger difference, often a sign of overfitting.\n",
    "- **Ideal Line (Dashed Diagonal):** Represents the ideal case where training and testing R² are equal, indicating perfect generalization.\n",
    "- **Zones:**\n",
    "  - *Ideal Zone:* High training and testing R² values, showing strong fit and good generalization.\n",
    "  - *Overfitting Zone:* High training R² but low testing R², suggesting the model fits the training data too closely but fails to generalize.\n",
    "  - *Underfitting Zone:* Low training and testing R², indicating poor fit and poor generalization.\n",
    "  - *High Generalization Zone:* Moderate training fit with high testing R², showing models that generalize well without overfitting.\n",
    "\n",
    "Annotations next to points identify the models for easy comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to visualize model generalization vs overfitting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scatter1 = plt.scatter(\n",
    "    model_results_df['R2'], \n",
    "    model_results_df['Test_R2'], \n",
    "    s=model_results_df['R2_Gap']*2500 + 100, \n",
    "    color='skyblue',\n",
    "    alpha=0.85,\n",
    "    edgecolors='black',\n",
    "    zorder=10\n",
    ")\n",
    "\n",
    "# Annotate each point with the model's short name\n",
    "for i, row in model_results_df.iterrows():\n",
    "    plt.annotate(row['ShortName'], \n",
    "                 (row['R2'] + 0.005, row['Test_R2'] - 0.005),\n",
    "                 fontsize=9, alpha=0.9)\n",
    "\n",
    "# Reference lines and zones for interpretation\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', alpha=0.5, label='Ideal Line')\n",
    "plt.axvline(0.85, color='red', linestyle=':', alpha=0.6)\n",
    "plt.axhline(0.85, color='red', linestyle=':', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Training R²', fontsize=12)\n",
    "plt.ylabel('Testing R²', fontsize=12)\n",
    "plt.title('Overfitting vs Generalization', fontsize=14, pad=15)\n",
    "\n",
    "# text annotations to define interpretation zones\n",
    "plt.text(0.92, 0.92, 'Ideal Zone\\n(High Fit & Generalization)', \n",
    "         fontsize=10, color='green', ha='center')\n",
    "plt.text(0.95, 0.75, 'Overfitting Zone\\n(High Fit, Low Generalization)', \n",
    "         fontsize=10, color='red', ha='center')\n",
    "plt.text(0.65, 0.75, 'Underfitting Zone\\n(Low Fit, Low Generalization)', \n",
    "         fontsize=10, color='orange', ha='center')\n",
    "plt.text(0.75, 0.95, 'High Generalization Zone\\n(Moderate Fit, High Generalization)', \n",
    "         fontsize=9, color='blue', ha='center')\n",
    "\n",
    "plt.xlim(0.55, 1.02)\n",
    "plt.ylim(0.55, 1.0)\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bfcc5",
   "metadata": {},
   "source": [
    " Summary: \n",
    "\n",
    "- **Gradient Boosting – Best Booster** is located in the **top-right corner**, achieving **high R² on both training and testing sets**, which reflects **excellent fit and generalization**.\n",
    "- While its **point size is not the smallest**, it maintains a **relatively small R² gap**, indicating **controlled overfitting** and a **strong trade-off between accuracy and generalization**.\n",
    "- **Random Forest** also performs well, balancing fit and generalization effectively.\n",
    "- **Linear Regression** has the **smallest R² gap** (smallest point size), but both its training and testing R² are **comparatively low**.\n",
    "- **Decision Tree** exhibits **very high training R²** but much lower test R², a clear sign of **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166066c",
   "metadata": {},
   "source": [
    "###  How to ReadTrain MSE vs Test MSE Scatter Plot\n",
    "\n",
    "This plot compares the MSE on training vs. testing data across models.\n",
    "\n",
    "- **X-axis (Train MSE):** Lower values indicate a better fit on training data.\n",
    "- **Y-axis (Test MSE):** Lower values indicate better performance on unseen data.\n",
    "- **Dashed Diagonal Line:** Represents perfect generalization (Train MSE = Test MSE).\n",
    "- **Point Position:**\n",
    "  - **Bottom-left corner:** Best models — low error on both training and testing data.\n",
    "  - **Above diagonal:** Overfitting — test error > train error.\n",
    "  - **Near diagonal:** Balanced generalization.\n",
    "- **Annotations:** Model names for direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd080d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scatter2 = plt.scatter(\n",
    "    model_results_df['MSE'],      \n",
    "    model_results_df['Test_MSE'], \n",
    "    s=200,                       \n",
    "    color='steelblue',           \n",
    "    edgecolors='black',\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "for i, row in model_results_df.iterrows():\n",
    "    plt.annotate(row['ShortName'], \n",
    "                 (row['MSE'] * 1.02, row['Test_MSE'] * 0.98),\n",
    "                 fontsize=9, alpha=0.9)\n",
    "\n",
    "max_mse = max(model_results_df['MSE'].max(), model_results_df['Test_MSE'].max()) * 1.05\n",
    "plt.plot([0, max_mse], [0, max_mse], '--', color='gray', alpha=0.5, label='Ideal Line')\n",
    "\n",
    "plt.xlabel('Train MSE', fontsize=12)\n",
    "plt.ylabel('Test MSE', fontsize=12)\n",
    "plt.title('Train vs Test MSE Comparison', fontsize=14, pad=15)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174c8ef",
   "metadata": {},
   "source": [
    "###  Summary\n",
    "\n",
    "- **Gradient Boosting – Best Booster** is the **closest to the bottom-left**, with **lowest Train and Test MSE**, reflecting **excellent accuracy and generalization**.\n",
    "- **Random Forest** is also very close to the bottom-left, indicating **strong performance and low error**.\n",
    "- **Decision Tree** exhibits **low Train MSE** but **very high Test MSE**, a classic sign of **overfitting** — it fits the training data too well but generalizes poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0992aa",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388fb49",
   "metadata": {},
   "source": [
    "In this study, we explored the relationships between the composition of concrete mixtures and their corresponding compressive strength through data-driven analysis. \n",
    "\n",
    "In **Chapter 1**, the problem under discussion is introduced and the objective of the study is defined.\n",
    "\n",
    "In **Chapter 2**, the dataset was first preprocessed to ensure consistency and completeness. Subsequently, the distribution of the target variable and the correlations between relevant features were examined to support informed modeling decisions.\n",
    "\n",
    "In **Chapter 3**, **cross-validation** was adopted as a fundamental evaluation strategy throughout the modeling process. Various regression models were assessed, beginning with basic **linear regression**, followed by **polynomial regression**, **feature selection**, and regularization techniques such as **Ridge and Lasso**. Finally, **tree-based methods** — including **Decision Tree**, **Random Forest**, and **Gradient Boosting** — were applied to further improve performance. \n",
    "\n",
    "In **Chapter 4**, the discussion part includes comparison based on table and a few plots, indicating that the **best result** is from the **gradient boosting model**. Combining both the feature importance ranking and the PDP analysis, it can be concluded that Age, Cement, and Superplasticizer are the **most critical predictors** in the model — not only due to their high importance scores but also because of their clearly interpretable and consistent effects on the target variable.\n",
    "\n",
    "If more time had been available, several improvements could have been pursued. \n",
    "\n",
    "Overall, this analysis provides valuable insights into the importance of feature interactions in predicting concrete strength and highlights the potential of machine learning methods in advancing material design and quality control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

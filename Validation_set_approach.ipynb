{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the packages and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data from the file Auto.csv and remove all columns that are not complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('./Data/Auto.csv',na_values='?')\n",
    "auto = auto.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the mpg from the horsepower in the auto dataframe. In the simple linear regression task we already saw, that the linear fit may not be the best choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Randomly split the data into a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "training = np.random.choice([False,True], size=392)\n",
    "\n",
    "y_train = auto.mpg[training]\n",
    "X_train = sm.add_constant(auto.horsepower[training])\n",
    "\n",
    "y_test = auto.mpg[~training]\n",
    "X_test = sm.add_constant(auto.horsepower[~training])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Fit a linear regression model on the training set and compute the MSE with the predictions on the validation set (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 20.8574\n"
     ]
    }
   ],
   "source": [
    "linear_model = sm.OLS(y_train, X_train)\n",
    "linear_results = linear_model.fit()\n",
    "\n",
    "y_predictions = linear_results.predict(X_test)\n",
    "mse = np.mean((y_predictions - y_test) ** 2)\n",
    "print(f'Mean Squared Error: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Fit a quadratic regression model on the training set and compute the MSE with the predictions on the validation set (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Quadratic): 16.4569\n",
      "The quadratic model has a lower MSE than the linear model.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.674\n",
      "Model:                            OLS   Adj. R-squared:                  0.670\n",
      "Method:                 Least Squares   F-statistic:                     209.4\n",
      "Date:                Thu, 12 Jun 2025   Prob (F-statistic):           4.47e-50\n",
      "Time:                        17:49:02   Log-Likelihood:                -608.65\n",
      "No. Observations:                 206   AIC:                             1223.\n",
      "Df Residuals:                     203   BIC:                             1233.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         58.5015      2.710     21.587      0.000      53.158      63.845\n",
      "x1            -0.4839      0.046    -10.466      0.000      -0.575      -0.393\n",
      "x2             0.0013      0.000      7.234      0.000       0.001       0.002\n",
      "==============================================================================\n",
      "Omnibus:                        3.244   Durbin-Watson:                   1.108\n",
      "Prob(Omnibus):                  0.198   Jarque-Bera (JB):                3.410\n",
      "Skew:                           0.108   Prob(JB):                        0.182\n",
      "Kurtosis:                       3.592   Cond. No.                     1.38e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.38e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.589\n",
      "Model:                            OLS   Adj. R-squared:                  0.587\n",
      "Method:                 Least Squares   F-statistic:                     292.9\n",
      "Date:                Thu, 12 Jun 2025   Prob (F-statistic):           2.67e-41\n",
      "Time:                        17:49:02   Log-Likelihood:                -632.27\n",
      "No. Observations:                 206   AIC:                             1269.\n",
      "Df Residuals:                     204   BIC:                             1275.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         40.0359      1.018     39.328      0.000      38.029      42.043\n",
      "horsepower    -0.1546      0.009    -17.113      0.000      -0.172      -0.137\n",
      "==============================================================================\n",
      "Omnibus:                        5.532   Durbin-Watson:                   0.984\n",
      "Prob(Omnibus):                  0.063   Jarque-Bera (JB):                5.512\n",
      "Skew:                           0.401   Prob(JB):                       0.0636\n",
      "Kurtosis:                       2.979   Cond. No.                         315.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Linear Model Coefficients:\n",
      "const         40.035885\n",
      "horsepower    -0.154576\n",
      "dtype: float64\n",
      "Quadratic Model Coefficients:\n",
      "const    58.501461\n",
      "x1       -0.483905\n",
      "x2        0.001274\n",
      "dtype: float64\n",
      "Linear Model R-squared: 0.5894\n",
      "Quadratic Model R-squared: 0.6736\n",
      "Linear Model P-values:\n",
      "const         3.529166e-97\n",
      "horsepower    2.665426e-41\n",
      "dtype: float64\n",
      "Quadratic Model P-values:\n",
      "const    1.803574e-54\n",
      "x1       8.933285e-21\n",
      "x2       9.434858e-12\n",
      "dtype: float64\n",
      "Linear Model Confidence Intervals:\n",
      "                    0          1\n",
      "const       38.028746  42.043023\n",
      "horsepower  -0.172385  -0.136767\n",
      "Quadratic Model Confidence Intervals:\n",
      "               0          1\n",
      "const  53.157980  63.844942\n",
      "x1     -0.575072  -0.392737\n",
      "x2      0.000927   0.001622\n",
      "Linear Model Residuals:\n",
      "1       0.469177\n",
      "2       1.150535\n",
      "4      -1.395226\n",
      "5       5.570190\n",
      "6       7.970865\n",
      "         ...    \n",
      "389     6.803424\n",
      "390     8.948510\n",
      "393    12.002074\n",
      "394     4.948510\n",
      "396     3.639358\n",
      "Length: 206, dtype: float64\n",
      "Quadratic Model Residuals:\n",
      "1      1.647417\n",
      "2      3.410361\n",
      "4      1.267059\n",
      "5      2.350300\n",
      "6      0.276871\n",
      "         ...   \n",
      "389    8.208566\n",
      "390    9.154402\n",
      "393    7.215618\n",
      "394    5.154402\n",
      "396    3.609692\n",
      "Length: 206, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train = sm.add_constant(np.column_stack((auto.horsepower[training], auto.horsepower[training]**2)))\n",
    "X_test = sm.add_constant(np.column_stack((auto.horsepower[~training], auto.horsepower[~training]**2)))\n",
    "\n",
    "quad_model = sm.OLS(y_train, X_train)\n",
    "quad_results = quad_model.fit()\n",
    "\n",
    "y_predictions_quad = quad_results.predict(X_test)\n",
    "mse_quad = np.mean((y_predictions_quad - y_test) ** 2)  \n",
    "print(f'Mean Squared Error (Quadratic): {mse_quad:.4f}')   \n",
    "# Compare the two models\n",
    "if mse_quad < mse:\n",
    "    print(\"The quadratic model has a lower MSE than the linear model.\") \n",
    "else:\n",
    "    print(\"The linear model has a lower MSE than the quadratic model.\")\n",
    "# Display the summary of the quadratic model\n",
    "print(quad_results.summary())\n",
    "# Display the summary of the linear model\n",
    "print(linear_results.summary())\n",
    "# Display the coefficients of the linear model\n",
    "print(\"Linear Model Coefficients:\")\n",
    "print(linear_results.params)\n",
    "# Display the coefficients of the quadratic model\n",
    "print(\"Quadratic Model Coefficients:\")\n",
    "print(quad_results.params)\n",
    "# Display the R-squared values of both models\n",
    "print(f\"Linear Model R-squared: {linear_results.rsquared:.4f}\")\n",
    "print(f\"Quadratic Model R-squared: {quad_results.rsquared:.4f}\")\n",
    "# Display the p-values of the coefficients for both models\n",
    "print(\"Linear Model P-values:\") \n",
    "print(linear_results.pvalues)\n",
    "print(\"Quadratic Model P-values:\")\n",
    "print(quad_results.pvalues)\n",
    "# Display the confidence intervals for the coefficients of both models\n",
    "print(\"Linear Model Confidence Intervals:\")\n",
    "print(linear_results.conf_int())\n",
    "print(\"Quadratic Model Confidence Intervals:\")\n",
    "print(quad_results.conf_int())\n",
    "# Display the residuals of both models\n",
    "print(\"Linear Model Residuals:\")\n",
    "print(linear_results.resid)\n",
    "print(\"Quadratic Model Residuals:\")\n",
    "print(quad_results.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Fit a cubic regression model on the training set and compute the MSE with the predictions on the validation set (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Cubic): 16.4427\n",
      "\n",
      "The cubic model has the lowest MSE among all models.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.674\n",
      "Model:                            OLS   Adj. R-squared:                  0.669\n",
      "Method:                 Least Squares   F-statistic:                     139.4\n",
      "Date:                Thu, 12 Jun 2025   Prob (F-statistic):           5.95e-49\n",
      "Time:                        17:51:30   Log-Likelihood:                -608.44\n",
      "No. Observations:                 206   AIC:                             1225.\n",
      "Df Residuals:                     202   BIC:                             1238.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         62.6276      6.962      8.996      0.000      48.900      76.355\n",
      "x1            -0.5943      0.178     -3.346      0.001      -0.945      -0.244\n",
      "x2             0.0022      0.001      1.542      0.125      -0.001       0.005\n",
      "x3         -2.252e-06    3.5e-06     -0.644      0.521   -9.15e-06    4.65e-06\n",
      "==============================================================================\n",
      "Omnibus:                        3.751   Durbin-Watson:                   1.123\n",
      "Prob(Omnibus):                  0.153   Jarque-Bera (JB):                4.312\n",
      "Skew:                           0.094   Prob(JB):                        0.116\n",
      "Kurtosis:                       3.683   Cond. No.                     6.21e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.21e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "X_train = sm.add_constant(np.column_stack((auto.horsepower[training], auto.horsepower[training] ** 2, auto.horsepower[training] ** 3)))\n",
    "X_test = sm.add_constant(np.column_stack((auto.horsepower[~training], auto.horsepower[~training] ** 2, auto.horsepower[~training] ** 3)))\n",
    "\n",
    "model = sm.OLS(y_train, X_train)\n",
    "cubic_results = model.fit()\n",
    "\n",
    "y_predictions_cubic = cubic_results.predict(X_test)\n",
    "mse_cubic = np.mean((y_predictions_cubic - y_test) ** 2)\n",
    "print(f'Mean Squared Error (Cubic): {mse_cubic:.4f}\\n')\n",
    "# Compare the cubic model with the previous models\n",
    "if mse_cubic < mse_quad and mse_cubic < mse:\n",
    "    print(\"The cubic model has the lowest MSE among all models.\")\n",
    "else:\n",
    "    print(\"The cubic model does not have the lowest MSE compared to the previous models.\")\n",
    "# Display the summary of the cubic model\n",
    "print(cubic_results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
